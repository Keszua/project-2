
# Trzeba sciagnac i zainstalowac dockera 
https://www.docker.com/get-started

# oraz WSL2  (taki linux na windowsa)
https://docs.microsoft.com/pl-pl/windows/wsl/install-manual#step-4---download-the-linux-kernel-update-package

# zakladanie konta na:
https://hub.docker.com/signup


#-------------------------------------------------------------------------------------------------
Instalacja na Linxa (Ubuntu)
https://docs.docker.com/engine/install/ubuntu/

Po zainstalowaniu, trzeba dodać do grupy użytkowników, żeby nie trzeba było za każdym razem dodawać sudo 
sudo usermod -aG docker $USER

#-------------------------------------------------------------------------------------------------
#-------------------------------------------------------------------------------------------------
#-------------------------------------------------------------------------------------------------


# link do repo z kursu: 
https://github.com/pnowy/docker-course
#-------------------------------------------------------------------------------------------------
docker version
docker info       # liczba kontenerow i obrazow


docker            # wyswietli liste polecen

docker image pull nazwaObrau         # sciagniecie danego obrazu, bez uruchamiania
docker image pull nazwaObrau:mojTag  # sciagniecie danego obrazu + nadaj tag ale tez jakos koles sciagal odpowiednie wersje za pomocc tagu (film 18)
docker run
docker container run                 # uruchomienie NOWEGO kontenera. Jezeli nie posiadamy obrazu, zostanie on sciagniety i uruchomiony
docker container run -p 8080:80 -d nginx  
#                     | |    |   | └ nazwa obrazu
#                     | |    |   └ detach: uruchomienie kontenera w tle
#                     | |    └ port po stronie kontenera
#                     | └ port, na ktorym kontener bedzie dostepny na naszym hoscie (zwykle localhost:8080)
#                     └ publish: przekierowanie, -p [host-port:container-port]
#  zatrzymanie kontenera przez:  Ctrl+C

docker container run -p 9200:9200 -p 9300:9300 -e "discovery.type=single-node" -d --name=myName docker.elastic.co/elasticsearch/elasticsearch:6.5.4
#                     |                         |                               |   |           |                                             └ wersja albo tag
#                     |                         |                               |   |           └ nazwa obrazu
#                     |                         |                               |   └ nazwa kontenera
#                     |                         |                               └ detach: Run container in background and print container ID
#                     |                         └ przekazanie zmiennej srodowiskowej
#                     └[container-port:host-port]


#                          HOST
#                           ┌─────────────┐
#  Port 3000 ---> Port 3000 │  Container  │
#                           └─────────────┘
#                           ┌─────────────┐
#  Port 3001 ---> Port 3000 │  Container  │
#                           └─────────────┘


docker stop idkontenera
docker container stop idkontenera


#  uruchom w trybie interaktywnym z terminalem (wyjscie komenda exit)
docker container run -it nginx bash 
docker container start -ai idkontenera
docker container exec -it idkontenera bash  #  odpalenie konsoli na dzialajacym kontenerze
docker container exec -it idkontenera sh    #  odpalenie konsoli na dzialajacym kontenerze dla alpine
docker attach  [id lub nazwa kontenera]     #  odpalenie konsoli na dzialajacym kontenerze. Przerywamy Ctrl+C (cos było że nie polecane)

# aby w apline ściągnąc nano: apk update && apk add nano 

docker container ls     #  wyswietli liste kontenerow URUCHOMIONYCH
docker ps               #  po staremu 
docker container ls -a  #  wyswietli liste wszystkich kontenerow, czyli nie uruchominych tez
docker image ls         #  lista obrazow

docker container start idkontenera  #  uruchominie istniejacego kontenera
docker rm idkontenera               #  usowanie kontenera
docker system prune --all           #  usowanie WSZYSTKICH obrazów jednym poleceniem:


docker container logs nazwaKontenera    # informacje o uruchamianiu kontenera
docker container logs nazwaKontenera -f # ciagla obserwacja (zablokuje CLI)


# curl - taka przegladarka z linii komend
curl -H "Content-Type: application/json" -XPOST "http://localhost:9200/docker/image/1" -d "{ \"name\" : \"elasticsearch\"}"
curl localhost:9200/docker/_search

#  do obserwowania, co sie dzieje w kolejkach
docker run -d --name rabbit-in-the-hole -p 8081:15672 rabbitmq:3-managment
#  po wejsciu na przegladarce, mozna sie zalogowac L: guest  H: guest


docker container top idkontenera      #  obserwacja procesow
docker container inspect idkontenera  #  wyswietla pelna konfiguracje kontenera (w formacie JSON)
docker container stats                #  "live striming" ogolny status wszystkich uruchominych kontenerow


#  -------------------------------------------------------------------------------------------------
docker network ls 
docker network inspect nazwaSieci

#  tworzenie sieci:
docker network create --driver=bridge mojaNowaNazwaSieci

#  uruchomienie kontenera "nginx" w konkretnej sieci (tutaj nazwa sieci to: skynet)
docker container run -d --network=skynet nginx
docker network connect skynet first      #  podlaczenie do sieci, kontenera first
docker network disconnect bridge first   #  odpiecie kontenera od domyslnej sieci (bo teraz byly dwie sieci)

#  Usuwanie sieci:
#  w pierszej kolejnosci trzeba odpiac lub usunac kontenery

#  DNS
#  Film 17 (Docker od podstaw)
#  Trzeba sprawdzic IPAddress  (polecenie docker network inspect ...)
#  Endpoint informujacy o stanie serwisu: (w konsoli linuxa)
curl http://172.17.0.3:8080/actuator/health

#  do wywołania postów curlem:
curl -X POST http://localhost:3000/parametr


#  urzycie opcji link ... na przykladzie polaczenia do bazy MSQL
docker run --name wordpressdb -e MYSQL_ROOT_PASSWORD=wordpress -e MYSQL_DATABASE=wordpress -d mysql:5.7
docker run -e WORDPRESS_DB_PASSWORD=wordpress -d --name wordpress --link wordpressdb:mysql -p 80:80  wordpress:5-php7.2
#                                   |                   |                |           |        └ port zewnetrzny dla przegladarki
#                                   |                   |                |           └ wewnetrzny alias, domyslnia nazwa dla bazy danych to mysql
#                                   |                   |                └ nazwa kontenera
#                                   |                   └ nazwa kontenera
#                                   └ nazwa bazy danych


# konfiguracja PrestaShop z bazą przez zmienną środowiskową (nie przez link)
# create a network for containers to communicate
$ docker network create prestashop-net
# launch mysql 5.7 container
$ docker run -ti --name some-mysql --network prestashop-net -e MYSQL_ROOT_PASSWORD=admin -p 3307:3306 -d mysql:5.7
# launch prestashop container
$ docker run -ti --name some-prestashop --network prestashop-net -e DB_SERVER=some-mysql -p 8080:80 -d prestashop/prestashop
# UWAGA! Gdy zmienie nazwę bazy, to tez trzeba ta nazwe zmienic w DB_SERVER=some-mysql

# kontener z zablokowaną siecią:
docker run --network none nginx


Nazwa sklepu: Karol_test
Imie: Karol
Nazwisko: M
Email: k.michalczyk@humansoft.pl
H 12345678

#  -------------------------------------------------------------------------------------------------
#  Budowanie obrazow

docker image history nazwaObrazu   # mozna podejzec, kiedy i z jakich warstwy zostal stworzony obraz
docker image inspect idKontenera   # wyswietla pelna konfiguracje obrazu (w formacie JSON)

#  nadawanie nowego tagu
docker image tag nazwa:zTagiem nazwaKontaDockerHub/nazwa:v001
#                                                  └ nazwa z ewentualnym tagiem
docker image tag nazwa:nazwa:v001 nazwaKontaDockerHub/nazwa:v001


#  wyslanie obrazu na wlasne repozytorium:
#  trzeba byc zalogowanym, te same dane co na https://hub.docker.com/ :
docker login 
docker image push nazwaKontaDockerHub/nazwaObrazu:zTagiem
docker logout #  jezeli chcemy sie wylogowac 

#  pobranie ze swojego repo
docker image pull nazwaKontaDockerHub/nazwaObrazu:zEwentualnymTagiem

#  tworzenie obrazu na podstawie kontenera:
docker container commit idKontenera nazwaKontaDockerHub/nazwaObrazu:zTagiem


  ####                         #      #                   #        
  #   #                        #      # #                 #        
  #   #  # ###  #####  #    #  #  #   ##      ####    #####  #    #
  ####   ##        #   #   #   # #   ##           #  #    #  #   # 
  #      #        #     # #    ##     #       #####  #    #   # #  
  #      #       #       #     # #    #   #  #    #  #    #    #   
  #      #      #####   #      #  #    ###    ### #   #####   #    
                       #                                     #    

# Alpine
docker container run --name Franek -it alpine:3.17 /bin/sh
docker container start -ai haszKontenera
# aby w apline ściągnąc nano: apk update && apk add nano 

# Przykladowe kontenery
docker container run -d -p 8080:80 --name mynginx nginx:latest
docker container run -d -e POSTGRES_USER=user1 -e POSTGRES_PASSWORD=pass123 -p 5444:5432 postgres:latest


# tworzenie sieci i uruchomienie kontenera wordpresa z mySql
docker network create --driver=bridge skynet
docker container run -d -p 3308:3306 --name db -e MYSQL_DATABASE=exampledb -e MYSQL_USER=exampleuser -e MYSQL_PASSWORD=12345678 -e MYSQL_RANDOM_ROOT_PASSWORD=1 --network=skynet --restart=always mysql:5:7 !!UWAGA - nie zadziałało
#                                                 └ Jak ma sie nazywac baza                                                        └ wygeneruj automatycznie bespieczne haslo dla roota
docker container run -d -p 8080:80 -e WORDPRESS_DB_HOST=db:3306 -e WORDPRESS_DB_USER=exampleuser -e WORDPRESS_DB_PASSWORD=12345678 -e WORDPRESS_DB_NAME=exampledb --network=skynet  --restart=always wordpress:latest
#                                                          └ port besposredni (nie do lokalnego przekierowania)

# kontener z sql (działa)
docker container run -d -p 3310:3306 --name black-box -e MYSQL_DATABASE=black_box -e MYSQL_ROOT_PASSWORD=admin -e -d mysql:5.7

# kontener dla mapy z MegaK (działa)
docker container run -d -p 3309:3306 --name mapaMegaK -e MYSQL_DATABASE=megak_ads -e MYSQL_PASSWORD=pass -e MYSQL_ROOT_PASSWORD=pass -e MYSQL_USER=karol -d mysql:5.7


# kontener z centosem:
docker run -it centos:8 bash
# aby naprawić instalację trzeba zrobić:
cd /etc/yum.repos.d/
sed -i 's/mirrorlist/#mirrorlist/g' /etc/yum.repos.d/CentOS-*
sed -i 's|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g' /etc/yum.repos.d/CentOS-*
yum update -y
yum install NetworkManager
# od teraz dzial polecenie:
nmcli -h


# Przykład kontenera z Postgesem - z filmu TechWorld with Nana: https://www.youtube.com/watch?v=GeqaTjKMWeY
docker run postgres:9.6




  #####                 #                     ###       #
   #   #                #                    #      #   #
   #   #   ###    ###   #  #   ###   # ###   #          #       ###
   #   #  #   #  #   #  # #   #   #  ##     ####   ##   #      #   #
   #   #  #   #  #      ##    #####  #       #      #   #      #####
   #   #  #   #  #   #  # #   #      #       #      #   #   #  #
  #####    ###    ###   #  #   ###   #       #     ###   ###    ###
#  -------------------------------------------------------------------------------------------------
#  Dockerfile
#  plik Dockerfile służy do tworzenia obrazów
#  w filmie 21 koles pokazuje plik Dockerfile gdzie definiuje sie, jak przerobic i skonfigurowac obraz
https://docs.docker.com/engine/reference/builder/


#  Przykład pliku:
┌───────────────────
FROM node:10-slim
WORKDIR /app             #  Create app directory WORKDIR to tak jak RUN cd /app   Dla noda: /app   dla nginx: /usr/share/nginx/html

COPY package*.json ./    #   Kopiuje pliki Z lokalnego host DO obrazu. Inny przyklad: COPY package.json package-lock.json /code/

RUN npm install --only=production   #   RUN wykonuje komendę

COPY . .   #   Kopiuj wszystko z folderu, oprucz elementów zdefiniowanych   lub: COPY src /code/src

EXPOSE 3000  #  port do nasłuchu, w innym przykladzie dla node byl port:  8080

ENV NODE_ENV production  #   Zmienne srodowiskowe przekazane do obrazu

#   ENTRYPOINT [ "entrypoint.sh" ]  Gdy nie podamy Entrypoint, wywoła się polecenie domyślne: /bin/sh -c
#   "entrypoint.sh" - to plik, który powninen być obok Dockerfile
#   CMD ["trafik"] specyfikuje argument, jaki będzie przekazywany do ENTRYPOINTa
CMD ["node", "app.js"]     # w innym przykladzie:  CMD [ "npm", "start" ]

LABEL   #  - metadane do obrazu
└───────────────────

# Zbudwanie obrazu (do odpalenia lokalnie)
docker image build -t nowaNazwaObrazu:tag .
#                   └ Name and optionally a tag in the 'name:tag' format
docker image build -t nowaNazwaObrazu:tag --progress=plain . # hyba robi symulacje budowania


# po zbudowaniu, mozna go uruchomic: docker container run -d -p 8080:8080 --name=nazwaKontenera nowaNazwaObrazu:tag
# można z flagą --no-cache, chyba żeby za każdym razem pobierało obraz?

#podgląd zmiennych środowiskoych wewnatrz kontenera:
printenv


  #     #         #
  #     #         #
  #     #   ###   #      #   #  ### ##    ###    ###
  #     #  #   #  #      #   #  #  #  #  #   #  #
   #   #   #   #  #      #   #  #  #  #  #####   ###
    # #    #   #  #   #  #   #  #  #  #  #          #
     #      ###    ###    ####  #  #  #   ###    ###
#--------------------------------------------------------------------------------------------------
#  Volumes
docker volume ls     #  wyświetl wszytkie volumeny
docker volume prune  #  usuwanie wszytkie volumeny


#  aby sprawdzić czy volumen jest podpięty, wywołac polecenie:
docker container inspect idKontenera
#  w "Mounts": [    powinien być "Type": "volume"  z jego nazwą (id)


# przykład ze strony https://adamtheautomator.com/docker-windows/
# Parametr --mount wymaga trzech argumentów; 
docker run --mount type=bind,source="E:/",target=/home/TEST -it alpine
#                  |         |            └ścieżka do katalogu docelowego. Ścieżka docelowa będzie dowiązaniem symbolicznym w kontenerze.
#                  |         └ścieżka do źródłowego katalogu hosta
#                  └typ montowania


#  aby do kontenera podpiąć istniejący wolumen:
docker container run -d --mount 'src=idVolumenu,dst=/appdata' -p 3000:3000 nazwaObrazu

#  starsza metoda podpinania volumenu, przez literę -v
docker container run -d -v idVolumenu:/appdata' -p 3000:3000 nazwaObrazu

#  uruchomienie kontenera i stworzenie volumenu z "ludzką" nazwą
docker container run -d --mount 'src=nazwa-volumenu,dst=/appdata' -p 3000:3000 nazwaObrazu

# przykład z filmu https://www.youtube.com/watch?v=p2PH_YPCsis&list=PLy7NrYWoggjzfAHlUusx2wuDwfCrmJYcs&index=12
docker run -v /home/mount/data:/var/lib/mysql/data
#              |                └ sciezka do katalogu w maszynie (wnętrze kontenera)
#              └ Host Volumes. Jezeli tego nie podam, doker wygeneruje losową nazwę: /var/lib/docker/volumes/random-hash/_data

#przykład na volumen do źródeł noda
docker run -v ${pwd}:/app:ro -v /app/node_modules --env-file ./.env -p 9000:3000 -d node-docker:v1.0.2
#               |         |   |                     └ zmienne środowiskowe można wypisać w pliku .env w posaci: PORT=3000
#               |         |   └ drugi volumen, to sztuczka na nie kopiowanie node_modules
#               |         └ tylko do odczytu
#               └ sciezka na TEN folder (z którego odpalamy): %cd% w cmd      ${pwd} w powershel    $(pwd) dla linux i mac


# Sztuczka na kopiowanie volumenu:
docker container run --rm -it -v staraNazwa:/from -v nowaNazwa:/to alpine ash -c "cd /from; cp -av . /to"


# Tworzenie kopii zapasowej danych Jenkinsa z kontenera do hosta https://adamtheautomator.com/jenkins-docker/
docker cp <container id>:/path/in/container /path/in/host
# przyklad:
docker cp my-jenkins-1:/var/jenkins_home ~/jenkins_backup


#--------------------------------------------------------------------------------------------------
#  Bind mounnts
#  coś jak Volumes dla wersji developerskiej (starsze podejście)

#  uruchamiamy kontener z flagą -v
docker run -d -p 80:80 -v $(pwd):/usr/share/nginx/html nginx
#  w przypadku windowsa:
docker run -d -p 80:80 -v //c/User/przemek/data:/path/data
#  u mnie zadziałało tak: (w konsoli bash)
docker run -d -p 80:80 -v //d/Karolek/Docker/dockerfiles/nginx-simple:/usr/share/nginx/html nginx



  #####                 #
   #   #                #
   #   #   ###    ###   #  #   ###   # ###        ###    ###   ### ##   ####    ###    ###    ### 
   #   #  #   #  #   #  # #   #   #  ##          #   #  #   #  #  #  #  #   #  #   #  #      #   #
   #   #  #   #  #      ##    #####  #           #      #   #  #  #  #  #   #  #   #   ###   #####
   #   #  #   #  #   #  # #   #      #           #   #  #   #  #  #  #  ####   #   #      #  #    
  #####    ###    ###   #  #   ###   #            ###    ###   #  #  #  #       ###    ###    ### 
                                                                        #
#--------------------------------------------------------------------------------------------------
#  Docker compose
#  Definicja w pliku: docker-compose.yml  #  może być inna nazwa, ale będzie trzeba ją podać w komendzie

#  ogólny pos:
version: '3.7'

services:               # definicja kontenerów (odpowiednik docker container run)

  db:         # nazwa serwisu (np. "db" lub "elasticsearch" ), będzie to także DNS serwisu w sieci
    image: mysql:5.7    # nazwa obrazu którego użyć do uruchomienia kontenera (opcjonalny w przypadku użycia build)
    restart: always     # always-po wykryciu bledu, restartuj do skutku.
    environment:        # zmienne środowiskowe przekazywane do kontenera przy jego uruchomieniu
      # KEY: value
      # KEY2: value2
      MYSQL_PASSWORD: db_password
      MYSQL_RANDOM_ROOT_PASSWORD: '1'  # dla automatycznego generowania hasla  # lub MYSQL_ROOT_PASSWORD: somewordpress
      MYSQL_DATABASE: wordpress
      MYSQL_USER: db_user
      MYSQL_PASSWORD: db_password
    env_file:           # zmienne środowiskowe
      - a.env           # zmienne środowiskowe z pliku
    command:            # nadpisanie domyślnego polecenia kontenera/obrazu
    volumes:            # odpowiednik -v z docker run (wsparcie zarówno starszej jak i nowszej składni)
      - db_data:/var/lib/mysql  # lub - db-vol:/var/lib/mysql
  
  mywordpress:          # kolejny serwis (np: "wordpress")
    image: wordpress:latest
    container_name: mywordpess   # OPCJONALNIE
    #command: # OPCJONALNIE, zastepuje CMD z Dockerfile
    restart: always
    depends_on:         # okreslamy zależność pomiędzy kontenerami. Ten jest zależny od "db", czyli uruchomi się, jak ten poprzedni już będzie działał
      - db

    ports:
      - "8000:80"
    environment:
      WORDPRESS_DB_HOST: db:3306   # adres serwera i port. Adres przez DNS jest taką nazwą jak nazwa kontenera
      WORDPRESS_DB_USER: db_user
      WORDPRESS_DB_PASSWORD: db_password
    volumes:
      - wordpress-volume:/var/www/html

volumes:                # definicja wolumenu (docker volume create)
  db_data:              # przyklad z nazwa volumenu (pusta nazwa po dwukropku (chyba))
  wordpress-volume:

networks:               # definicja sieci (docker network create). Jesli tego nie podamy, to zostanie stworzona nowa siec


Przykałdowy wordpress
localhost:8000
Użytkownik: karol
H: karol


# Aby uruchomić:
# W konsoli upewnij sie ze jestes na dobrej sciezce i uruchom poleceniem:
docker-compose up       # tworzy i uruchamia. Jeśli korzytam z DockerDesktop lub RangerDesktop, moge urzyc skladni ze spacja:  docker compose
docker-compose up -d    # uruchomi kontenery w tle
# jeśli nie uruchomiony w trybie -d, to konczymy Ctrl+C
docker-compose start    # uruchomienie stworzonych kontenerów
# jeśli w trybie -d, zatrzymujemy komendą:
docker-compose stop     # mozna urzyc z nazwa konkretnego, pojedynczego kontenera
docker-compose down     # zatrzymanie i usunięcie wszytkich kontenerów i zależności
docker-compose down -v  # zatrzymanie i usunięcie wszytkich kontenerów i zależności + usun volumeny

docker-compose ps       # liste kontenerów
docker-compose top      # obserwacja procesów
docker-compose logs     # obserwacja logów


#--------------------------------------------------------------------------------------------------
#Przykład pliku dla App Service i MySQL z kursu: https://docs.microsoft.com/pl-pl/visualstudio/docker/tutorials/use-docker-compose

version: "3.7"

services:
  # docker run -dp 3000:3000 -w /app -v ${PWD}:/app --network todo-app -e MYSQL_HOST=mysql -e MYSQL_USER=root -e MYSQL_PASSWORD=secret -e MYSQL_DB=todos node:12-alpine sh -c "yarn install && yarn run dev"
  # Definiowanie App Service.
  app:                                 # Nazwa serwisu (dowolna, ale ona będzie aliasem sieciowym)
    image: node:12-alpine              # wpis usługi i obraz kontenera
    command: sh -c "yarn install && yarn run dev"   
    ports:
      - 3000:3000                      # zmigrowane polecenie: -p 3000:3000
    working_dir: /app                  # zmigruj katalog roboczy ( -w /app )
    volumes:
      - ./:/app                        # mapowanie woluminów ( -v ${PWD}:/app )  Docker Compose używa ścierzek względnych
    environment:                       # zmienne środowiskowe
      MYSQL_HOST: mysql
      MYSQL_USER: root
      MYSQL_PASSWORD: secret
      MYSQL_DB: todos

  # docker run -d --network todo-app --network-alias mysql -v todo-mysql-data:/var/lib/mysql  -e MYSQL_ROOT_PASSWORD=secret -e MYSQL_DATABASE=todos mysql:5.7
  # Definiowanie usługi MySQL
  mysql:                               # nazwa: mysql
    image: mysql:5.7                   # określ obraz do użycia.
    volumes:                           # zdefiniuj mapowanie woluminów.
      - todo-mysql-data:/var/lib/mysql # określić punkt instalacji w konfiguracji usług
    environment: 
      MYSQL_ROOT_PASSWORD: secret
      MYSQL_DATABASE: todos


volumes:                          
  todo-mysql-data: 

# Uruchamianie stosu aplikacji:
# Najpierw upewnij się, że nie są uruchomione żadne inne kopie aplikacji i bazy danych ( docker ps i docker rm -f <ids> ).
# Uruchom stos aplikacji przy użyciu polecenia docker-compose up  Dodaj -d flagę , aby uruchomić wszystko w tle. Alternatywnie możesz kliknąć prawym przyciskiem myszy plik Compose i wybrać opcję Utwórz dla VS Code bocznym.
docker-compose up -d

docker-compose logs -f       # Przyjrzyj się dziennikom za pomocą polecenia 
docker-compose logs -f app   # dziennik dla konkretnej usługi

docker-compose down          # zatrzymanie kontenerów i usówanie sieci


#--------------------------------------------------------------------------------------------------
# Przykład działającej konfiguracji PrestaShop:
version: '3.9'

services:
    prestashop_mysql:
        # image: mysql:5.7
        image: prestashop-db:5.7_3
        container_name: prestashop-db
        command: --default-authentication-plugin=mysql_native_password
        environment:
            MYSQL_DATABASE: prestashop
            MYSQL_ROOT_PASSWORD: admin
        volumes:
            # - ./.docker/data/mysql/:/var/lib/mysql
            # - ./.docker/logs/mysql/:/var/log/mysql
            - db_data:/var/lib/mysql
        ports:
            - 3307:3306
        networks:
            - presta-net
        
    prestashop:
        # image: prestashop/prestashop:1.7
        image: prestashop:1.7_3
        container_name: prestashop
        ports:
          - 8080:80
          - 2202:2202
        environment:
            DB_SERVER: prestashop_mysql
            MYSQL_HOST: mysql
            MYSQL_USER: root
            MYSQL_PASSWORD: admin
            MYSQL_DB: prestashop
        networks:
            - presta-net

networks:
  presta-net:
    external: false #needs to be created by other file
    driver: bridge
    name: prestashop-net

volumes:
  db_data:
    name: docker_test1_db_data


#--------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------

https://github.com/veggiemonk/awesome-docker

#  poczytac o Kubernetes znany też jako: K8s
https://kubernetes.io/


Instalacja Minikube : https://www.youtube.com/watch?v=X48VuDVv0do&t=453s   40:23


https://ihermes.humansoft.pl/login


#--------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------
przez SSH:
docker1

VM 317

portainer.io
https://10.10.10.186:9443/#!/home
admin  
docker123


Nginx Proxy Manager
http://10.10.10.186:81/
m.zalecki@humansoft.pl
docker123


#--------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------

# jakiś filmik, jak stworzyć front w React i backend w Node.js  https://www.youtube.com/watch?v=-pTel5FojAQ

# W jednym folderze normalnie instaluje React
npx create-react-app forntend
# Robie drugi folder backend (na tym poziomie co cały React). 
# Wchodze do tego folderu, tam wywołuje 
npm init
# powstanie plik package.json, do kturego dopisuje zalezności (stare scripts usunąć):
{
  ...
  "dependencies": {
    "express": "4.16.3",
    "pg": "8.0.3",
    "cors": "2.8.4",
    "nodemon": "1.18.3",
    "body-parser": "*"
  },
  "scripts": {
    "dev": "nodemon",
    "start": "node index.js"
  }
}
# wywołujemy instalecję, aby zalezności się zainstalowały:
npm i

# Utwórz plik keys.js ze zmiennymi potrzebnymi dla kontenera
# Zawartość pliku:
module.exports = {
    pgUser: process.env.PGUSER,
    pgHost: process.env.PGHOST,
    pgDatabase: process.env.PGDATABASE,
    pgPassword: process.env.PGPASSWORD,
    pgPort: process.env.PGPORT,
}
# Utwórz plik index.js z prostym serwerem

#  uruchomienie serwera poleceniem:
npm run dev


# FRONT w package.json reacta do zalezności dopisujemy:   Film 9:20  https://www.youtube.com/watch?v=-pTel5FojAQ
{
  ...
  "dependencies": {
    ...
    "axios": "0.18.0",
    "react-router-dom": "4.3.1"
  },
}

# Projekt reacta odpalam przez:
npm run start


# Aby wsadzić projekt reacta  do kontenera, trzeba dodac plik (na poziomie .gitignore) Deckerfile.dev
# Zawartość pliku:   Film YT 18:30  https://www.youtube.com/watch?v=-pTel5FojAQ
FROM node:14.14.0-alpine
WORKDIR /app
COPY ./package.json ./
RUN npm i
COPY . .
CMD ["npm", "run", "start"]

# Buduje obraz projektu:
docker build -f Dockerfile.dev -t stylerhun/multi-client .

# Odpalam kontener:
docker run -it -p 4002:3000 --name=P1Front stylerhun/multi-client


# Teraz tworze kontener dla backendu, plik Dockerfile.dev jest podobny:
FROM node:14.14.0-alpine
WORKDIR /app
COPY ./package.json ./
RUN npm i
COPY . .
CMD ["npm", "run", "dev"]

# Buduje obraz projektu:
docker build -f Dockerfile.dev -t stylerhun/multi-server .

# Odpalam kontener:
docker run -it -p 4003:5000 --name=P1Back stylerhun/multi-server

#--------------------------------------------------------------------------------------------------
# Przykład 2 (DZIAŁAJĄCY)
# Jak skonteneryzować node.js   https://www.youtube.com/watch?v=CsWoMpK3EtE

# 1. Tworze nowy, pusty folder z projektem. Inicjalizuje projekt
npm init
# 2. Instaluje expres
npm install --save express
# 3. Edytuje package.json, dopisuje do "scripts": {
  "start": "node app.js",              # lub nodemon, ale to wymaga dodania kilku linijek (patrz niżej)
# 4. jeśli wszsytko jest OK, to powinien uruchomic się serwer. Po wejściu na localchost:3000 powinien być widoczny komunikat "Hello world"
npm run start
# 5. tworze plik Dockerfile  (bez rozszeżenia)
FROM node:9-slim
WORKDIR /app
COPY package.json ./app
RUN npm install
COPY . /app
CMD ["npm", "start"]
# UWAGA! dla kontenera node na debianie i alpine, trzeba zmienić ścierzkę na '.':
FROM node:18.3
#FROM node:18.3-alpine3.15   do wyboru
WORKDIR /app
COPY package*.json .    # <--- tu zmiana!
RUN npm install -g nodemon  # gdy chce korzystac z nodemona. Nie działa w wersji slim (dział w alpine)
RUN npm install
COPY . /app
CMD ["npm", "start"]
RUN apt-get update && apt-get install -y --force-yes nano  # gdy chce od razu korzystac z nano (nie działa w alpine)
RUN apk update && apk add nano                             # to tylko w alpine
#6. Musze byc na poziomie pliku Dockerfile. Tworze obraz poleceniem 
docker build -t node-docker-tutorial .
#7. Powinien stworzyć sie obraz. Bedzie widoczny po wpisaniu komendy  
docker image ls
#8. Uruchamiam kontener 
docker run -it -p 9000:3000 node-docker-tutorial # uruchomienie na chwilę (do puki w konsoli naciśniemy Ctrl+C)
docker run -d -p 9000:3000 node-docker-tutorial  # uruchomienie w tle 
#9. Aby zrobić automatyczne przeładowywanie zawartości plików, trzeba w package.json zmienić:
    # "start": "node app.js", na: "start": "nodemon app.js",
    # do dependencies dodać "nodemon": "^2.0.16"
    # UWAGA - u mnie coś nie zadziałało z tym nodemonem na slim. Ale zadziałało na wersji pełnej i alpine
# uruchominienie z volumenem:
docker run -v $(pwd):/app -d -p 9000:3000 node-app


# jeśli wejde do kontenera i tam zrobie zmiany, moge podejrzeć procesy
ps aux

#--------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------
  #####                 #
   #   #                #
   #   #   ###    ###   #  #   ###   # ###        ###   #     #   ####   # ###  ### ## 
   #   #  #   #  #   #  # #   #   #  ##          #      #     #       #  ##     #  #  #
   #   #  #   #  #      ##    #####  #            ###   #  #  #   #####  #      #  #  #
   #   #  #   #  #   #  # #   #      #               #  # # # #  #    #  #      #  #  #
  #####    ###    ###   #  #   ###   #            ###    #   #    ### #  #      #  #  #
#--------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------
Wspomniane w filmie https://www.youtube.com/watch?v=9zUHg7xjIqQ 
Oriestracja kontenerów za pomocą docker swarm
Informacja, czy jest uruchomiony, można sprawdzić poleceniem
	docker info
Jest tam wpis:
Swarm: inactive

Trzeba sprawdzić ip zdalnej maszyny eth0
	ip addr
Aktywacja docker swarma:
	docker swarm init --advertise-addr ipMaszynyzeth0

Komendy swarmose:
	docker ┬ services
  		   ├ stack    // do uruchomienia kontenerów w staku (taki zbiór). Pogląd ls
  		   ├ node     
  		   ├ stack    // takie logi
  		   ├
  		   └

	docker stack ls                   // podgląd staków
	docker stack services myapp ls    // podgląd kontenerów w staku
	docker services                   // podgląd serwisów
	docker stack ps myapp


Uruchomienie kontenerów oraz aktualizacja przez docker swarm (nie docker-compose)
	docker stack deploy -c docker-compose.yaml -c docker-compose.prod.yaml mojaNazwaStaka





#--------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------
  #   #           #                                         #                           .
  #  #            #                                         #                           .
  # #     #   #   ####     ###    # ###   ####     ###    #####    ###     ###          .
  ##      #   #   #   #   #   #   ##      #   #   #   #     #     #   #   #             .
  # #     #   #   #   #   #####   #       #   #   #####     #     #####    ###          .
  #  #    #   #   #   #   #       #       #   #   #         #     #           #         .
  #   #    ####   ####     ###    #       #   #    ###       ##    ###     ###          .
#--------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------
KUBERNEST

# Proponowana wtyczk: YAML Language Support by Red Hat, with built-in Kubernetes syntax support 
# Następnie trzeba wejsc do ustawien tej wtyczki, znaleźć "Yaml:Schemas" -> Edit in settings.json
# dopisać:
    "yaml.schemas": {
        "kubernetes": "*.yaml"
    }

# chymura na google:
https://cloud.google.com/free/docs/gcp-free-tier


komendy na: https://kubernetes.io/docs/reference/kubectl/

kubectl version  # sprawdzenie, czy jest zainstalowany kubernetes i w jakiej wersjii
kubectl version --client --output=yaml    # 

# tworzenie podów zdefiniowanych w folderze k8s ( trzeba być na takim poziomie, żeby po wpisaniu ls widac było folder 'k8s')
kubectl apply -f k8s

╔════════════════════════════════════════════════════════════════════════════════════════════════════╗
║                                       Kubernetes Cluster                                           ║
║ ╔══════════════════════╗    ╔═════════════════════════════════════════════════════╗                ║
║ ║ Control plane        ║    ║ Data plane                                          ║                ║
║ ║ (mózg Kubernetesa)   ║    ║ (worker nodes)    kubelet     Kube-proxy            ║                ║
║ ║                      ║    ║                     Container Runtime Engine        ║                ║  
║ ║  ┌────────────────┐  ║    ║  ┌─────────────────────────────┐    ┌─-----------┐  ║                ║ 
║ ║  │  Master        │  ║    ║  │ Node                        │    │ Node       │  ║                ║ 
║ ║  │────────────────│  ║    ║  │ ┌─────────────┐ ┌-----┐     │                    ║                ║ 
║ ║  │ETCD CLUSTER    │  ║    ║  │ │    POD      │   POD       │    │            │  ║                ║ 
║ ║  │kube-apiserver  │  ║    ║  │ │┌-----------┐│ │     │     │                    ║                ║ 
║ ║  │kube-controller-│  ║    ║  │ ││ localhost ││             │    │            │  ║                ║ 
║ ║  │  -manager      │  ║    ║  │ │ ┌─────────┐││ │     │     │                    ║                ║ 
║ ║  │kube-scheduler  │  ║    ║  │ │││Kontener │ │             │    │            │  ║                ║ 
║ ║  └────────────────┘  ║    ║  │ │ └─────────┘││ │     │     │                    ║                ║ 
║ ║  ┌────────────────┐  ║    ║  │ ││┌─────────┐ │             │    │            │  ║                ║ 
║ ║  │ Master         │  ║    ║  │ │ │Kontener │││ │     │     │                    ║                ║
║ ║  └────────────────┘  ║    ║  │ ││└─────────┘ │             │    │            │  ║                ║
║ ║  ┌────────────────┐  ║    ║  │ │└───────────┘│             │                    ║                ║
║ ║  │ Master         │  ║    ║  │ └─────────────┘ └-----┘     │    │            │  ║                ║
║ ║  └────────────────┘  ║    ║  │                             │                    ║                ║
║ ║                      ║    ║  └─────────────────────────────┘    └------------┘  ║                ║
║ ╚══════════════════════╝    ╚═════════════════════════════════════════════════════╝                ║
╚════════════════════════════════════════════════════════════════════════════════════════════════════╝

Control plane - przyjmuje rozkazy od użytkownika, monitoruje cały stan, reaguje na wydażenia, decyduje, gdzie kontener ma byc uruchomiony
Worker odpowiada za uruchamianie kontenerów (tu beda dzialac nasze aplikacje)

Skalowanie ze względu na dochodzacych urzytkowników odbywa się przez dodawanie/odejmowanie POD'ów
Gdy skończy się przepustowość, a urzytkowników nadal przybywa, skalowanie odbedzie się przez Node

Kube-scheduler - on decyduje, w kórym nodzie stworzyć nową instancję poda, podczas wużego obciążenia serwera

Controller manager - odzysuje uszkodzone pody

ETCD -jakis mózg do zarzadzania danymi (ETDC is a distributed reliable key-value store that is Simple, Secure i Fast)

Ingress - Chyba do zarządzania kontenerami z bazą danych. Decyduje gdzie trafi Request od urzytkownika. Do którego poda z baza danych 

         Request                     ┌─ my-app          ┌─ DB
Ingress ─────────> my-app Service ->─┼─── DB Service ──>┤
                                     └─ my-app          └─ DB



#--------------------------------------------------------------------------------------------------
kube-apiserver




#--------------------------------------------------------------------------------------------------
#            [LISTA]
kubectl get ┬ all            # pokaze pody, serwisy 
            ├ pods           # lista podów  z -A pokazuje ukryte systemowe
            ├ nodes          # lista wezlow
            ├ svc            # services  # lista serwisów z IP oraz portami
            ├ deployments    # aplikacja
            ├ ns             # namespaces
            ├ rc             # replicationcontrollers
            ├ rs             # replicaSet
            ├ job            # job, to taki jednorazowy pod
            ├ cronejob
            ├ configmaps
            ├ secrets
            ├ ds             # DaemonSet (zawsze, automatycznie uruchamiane pody)
            ├ endpoinds      # serwis tworzy taki endpoint przypisany do poda
            ├ serviceaccount #
            ├ persistentvolume
            ├ persistentvolumeclaim
            ├ configmap
            ├ ingress
            ├ serviceaccount # jakieś automatyczne pody
            └ statefulset    # taki deployment dla baz mysql
kubectl get [LISTA] -o widge # troszke wiecej informccji
kubectl get [LISTA] -o yaml  # szczegóły
kubectl describe [LISTA] [NAME] # szczegóły konkretnego poda/noda.  NAME - to nazwa z wyświetlonego polecenia, no get pods
kubectl cluster-info         # informacje o klastrze

kubectl edit [LISTA] nazwa-elementu

watch -n 1 kubectl get pods  # stały podgląd 


# Doinstalowanie automatycznego podpowiadania
sudo apt update
sudo apt install bash-completion
# instrukcje do dalszego działania sa pod komendą:
kubectl completion -h
#jednorazowe podpowiadanie (na czas sesji)
source <(kubectl completion bash)



# na OVH działa usługa kubelet
systemctl status kubelet

# klucze i certyfikaty sa ukryte pod poleceniem
sudo docker ps | grep etcd



# pliki definicji .yaml  lub .yml
# zawsze zawiera 4 podstawowe pola:
apiVersion: v1  # wersja kubernetes
kind:           # rodzaj
metadata:
spec:

#--------------------------------------------------------------------------------------------------
  ####                   #
  #   #                  #
  #   #     ###      #####
  ####     #   #    #    #
  #        #   #    #    #
  #        #   #    #    #
  #         ###      #####
#--------------------------------------------------------------------------------------------------
pod-ngix.yaml
┌───────────────────
apiVersion: v1                    # String
kind: Pod                         # String
metadata:                         #
  name: myapp-pod                 #
  labels:                         # etykiety. 
    app: myapp                    #
    tier: frontend                # etykieta do zgrupowania. Można nadać wiele etykiet.
    #env: production              # szukanie po etykiecie: kubectl get pods --selector tier=frontend
    #function: front-end
spec:
  containers:                     # List/Array
  - name: nginx-container         # pierwszy kontener. Dowolna nazwa
    image: nginx                  # nazwa z DockerHuba
    command: [ "sleep", "inf"]    # [opcjonalnie]. Te opcje "przytrzymują kontener przy życiu"
    ports:
      - containerPort: 80
  - name: ubuntu                  # opcjonalnie kolejny kontener
    image: ubuntu:18.04
    command: [ "/bin/sh", "-c" ]
    args: [ "sleep 5; exit 1" ]   # to ustawienie ma zasymulować błędne zakończenie programu, Nie urzyzwac tego
    env:                          # [opcjonalnie] zmienne środowiskowe
    - name: LINK_MY               # mozna je podejzec poleceniem: kubectl logs nazwaUruchomionegoPoda
      value: "https://strefakursow.pl"
    - name: LICZBA_KURSOW
      value: "100" 
    resources:                    # [opcjonalnie] https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/
      requests:
        memory: "1Gi"             # domyślnie 256Mi.   1Ki = 1,024B;  1Mi = 1,048,576B;  1Gi = 1,073,741,824B
        cpu: 1
      limits:                     # gdy osiągnie limit, pod zostanie usunięty z noda
        memory: "2Gi"
        cpu: 2
  - name: ubuntu-sleeper          # przykład z Film 96  Certified Kubernetes Administrator (CKA) with Practice Tests
    image: ubuntu-sleeper       
    command: ["sleep2.0"]         # odpowiednik pola ENTRYPOINT ["sleep"]   w dokerze
    args: ["10"]                  # odpowiednik pola CMD ["10"]             w dokerze
    env:       
    - name: APP_COLOR             # Plain Kye Value
      value: pink       
    - name: APP_COLOR             # ConfigMap     Musimy mieć poda z defincja kluczy
      valueFrom:                  #                  
        configMapKeyRef:          # Metoda dobra dla tylko pojedynczych kluczy          
          name: app-config        #
          key: APP_COLOR
    - name: APP_COLOR             # Secrets
      valueFrom:
        secretKeyRef:
          name: app-secret       
          key: pass               

    envFrom:                      # 
    - configMapRef:               # ConfigMap        APP_COLOR: blue
        name: app-config          #                  APP_MODE: prod
    - secretRef:
        name: app-secret

  - name: simple-webapp           # Przykład na sprawdzanie, czy aplikacj żyje 
    image: simple-webapp          # Film 72 Kubernetes Certified Application Developer (CKAD) with Tests
    ports: 
      - containerPort:8080
    ┌─────────
    livenessProbe:                # HTTP Test - /api/ready
      httpGet:
        path: /api/healthy
        port: 8080
      initialDelaySeconds: 10
      periodSeconds: 5 
      failureThreshold: 8
    └─────────
    ┌─────────
    livenessProbe:                # TCP Test - 3306
      tcpSocket:
        port: 3306
    └─────────
    ┌─────────
    livenessProbe:                # Exec Command
      exec:
        command:
          - cat
          - /app/is_healthy  
    └─────────

  restartPolicy: Always / Never / OnFailure  # [opcjonalnie] kiedy ma sie zrestartować
  nodeName:                       # [opcjonalnie]
  tolerations:                    # [opcjonalnie] dla taint nodes, gdy urzywamy kilku nodów i chcemy zrobić filtr, gdzie moga się instalować
    - key: "app"
      operator: "Equal"
      value: "blue"
      effect: "NoSchedule"        # co się stanie, gdy pod nie spełnia warunków: NoSchedule | preferNoShedule | NoExecute 
  nodeSelector:                   # [opcjonalnie]
    size: Large
  affinity:                       # [opcjonalnie] zamiast nodeSelector
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          key: size
          operator: In            # In | NotIn | Exists   In-jakie mogą wchodzić (można podac kilka);  NotIn-jakie mają nie wpadać;  Exists-bezpodawania values:
          values:
          - Large
          #- Medium
          #- Small
      
      #preferredDuringSchedulingIgnoredDuringExecution:
      #requiredDuringSchedulingRequiredDuringExecution:
  #serviceAccountname: dashboard-sa   # wspomniane coś w Film 52  Kubernetes Certified Application Developer (CKAD) with Tests
  
  readinessProbe:                 # [opcja] aby status był ready, dopiero gdy wszystko się uruchomi.
    ┌─────────                    # Wiecej na Film 71  Kubernetes Certified Application Developer (CKAD) with Tests
    httpGet:                      # HTTP Test - /api/ready
      path: /api/ready
      port: 8080
    initialDelaySeconds: 10
    periodSeconds: 5
    failureThreshold: 8           # domyślnie jest 3 sprawdzane. Możemy to przedłużyć.
    └─────────
    ┌─────────                    # TCP Test - 3306
    tcpSocket:  
      port: 3306
    └─────────
    ┌─────────                    # Exec Command
    exec:  
      command:
      - cat
      - /app/is_ready
    └─────────



└───────────────────

# stworzenie poda:   To polecenie również aktualizuje zmiany wprowadzone w pliku .yaml
kubectl create -f pod-definition.yml        # uruchomienie imperatywne

kubectl apply pod myapp-pod                 # uruchomienie deklaratywne poda (zwykle, gdy wprowadzimy zmiany w pliku .yaml)
kubectl apply -f pod-ngix.yaml              # uruchomienie deklaratywne poda (zwykle, gdy wprowadzimy zmiany w pliku .yaml)

# Tworzenie poda w konsoli, bez pliku .yaml Tutaj pod z kontenerem ngix
kubectl run nginx --image=nginx

kubectl get pods --selector app=App1        # te selektroy sa w metadata.labels[]

kubectl delete pod myapp-pod                # kasowanie poda

kubectl exec -it nazwa-poda bash                    # ─┬─ jak wejść do środka kontenera w podzie
kubectl exec --stdin --tty nazwa-poda -- /bin/bash  # ─┘
kubectl exec -c nginx -it nazwa-poda bash           # gdy mamy kilka kontenerów
kubectl exec nazwaPoda -- cat /log/app.log          # jednorazowe wyświetlenie zawartości pliku z kontenera

kubectl logs nazwa-poda                     # wyswietli logi   -f automatyczne odświerzanie
kubectl logs nazwa-poda -c nazwaKontenera   # wyswietli logi dla konkretnego kontenera

kubectl edit pod nazwaUruchomionegoPoda     # edycja uruchomionego poda
kubectl replace --force -f /tmp/kubectl-edit-834939467.yaml  # w przykładzie było, ze po edycji poda, trzeba było zrestartowac plik

# cykl tworzenia poda:
PodScheduled
Initialized
ContainersReady
Ready


Aby stworzyć plik .yaml na podstawie istniejącego już poda
kubectl get deployment nginx-deployment -o yaml > nginx-deployment-result.yaml

#--------------------------------------------------------------------------------------------------
Taint 
kubectl taint nodes node-name key=value:taint-effect       # wzornik
kubectl taint nodes nodeMyApp app=blue:NoShedule           # przykład

# aby sprawdzić, czy node posiada zdefiniowane filtry Taint:
kubectl describe node nazwaNoda | grep Taint

kubectl label nodes <node-name> <label-key>=<label-value>  # wzornik
kubectl label nodes node-1 size=Large                      # przykład

Node Affinity

#--------------------------------------------------------------------------------------------------
   ####                    #                               ###              #  
   #   #                   #        #                     #   #             #
   #   #    ###    ####    #              ###     ####    #        ###    #####
   ####    #   #   #   #   #       ##    #   #        #    ###    #   #     #
   # #     #####   #   #   #        #    #        #####       #   #####     #
   #  #    #       ####    #   #    #    #   #   #    #   #   #   #         #
   #   #    ###    #        ###    ###    ###     ### #    ###     ###       ##
                   #
#--------------------------------------------------------------------------------------------------
Replica Set - przywraca uszkodzone kontenery. Dba o to, aby okreslona liczba podow dzialala. Rowniez pomaga w skalowaniu podow
Replication Controller - podobne narzedzie jak Replica Set, ale już nie rekomedowany.
ReplicaSet trzyma ostatnio działające DEPLOYMENTY przy podnoszeniu wersji za pomocą   kubectl set image deployment nazwa-deploymentu httpd=httpd:2.4 --record=true 
ReplicaSet może istnieć (kubectl get rs) i mieć 0 uruchomionych uszkodzonych kontenerów. Po to, aby przywrócić działajaca wersję poleceniem   kubectl rollout undo deployment nazwa-deploymentu

Przykładowy plik ReplicaSet
┌───────────────────
apiVersion: apps/v1
kind: ReplicaSet  # ReplicationController
metadata:
  name: rs-webapp
  labels:
    app: myapp
    type: front-end
  #annotations:
  #  buildversion: 1.34  

spec:
  replicas: 3                # ilosc podow, iloma ma sie opiekowac
  
  template:                  # jaki pod ma byc uruchomiony w ramach tego replicasetu
    metadata:                          # \
      labels:                          # |
        app: myapp                     # |
    spec:                              # ├ Zawartosc taka jak w Pod
      containers:                      # |
      - name: webapp                   # |
        image: k8smaestro/web-app:1.0  # /
        #ports
        #- containerPorts: 80
  selector:                  # jakimi konkretnie podami ma sie zaopiekowac (filtruje nazwy podawane w "labels" )
    matchLabels:             # matchLabels albo matchExpressions
      type: front-end
      tier: front-end        # etykieta do grupowania. (To tylko na kursie Udemy)
      app: myapp             # tu odajemy nazwe, jaka jest w pliku definujacym poda:
                              ┌────────────────────────┐      
                              │ #pod-definition.yml     \    
                              │ apiVersion: v1           \    
                              │ kind: Pod                 \   
                              │ metadata:                  \ 
                              │   name:  myapp-pod          │ # (to tylko na kursie Udemy)
                              │   labels:                   │
                              │     app: myapp   <---       │
                              │     tier: front-end         │ # (to tylko na kursie Udemy)
                              └─────────────────────────────┘
└───────────────────

# stworzenie Repliki
kubectl create -f rc-definition.yml

# sprawdzenie istniejących replik:
kubectl get rc

# przeładowanie pliku
kubectl replace -f rc-definition.yml

# przeskalowanie ilosci replik
kubectl scale --replicas=6 -f rc-definition.yml       # lub:
kubectl scale replicaset myapp-replicaset --replicas=2 
kubectl scale rs --replicas=6 myapp-replicaset 
#             └type           └name


# edytowanie (u mnie stacjonarnie nie zadzialalo)
kubectl edit replicaset myapp-replicaset


# usuwanie
kubectl delete replicaset myapp
kubectl delete replicaset rs-webapp
kubectl delete -f rc-definition.yml


#--------------------------------------------------------------------------------------------------
#  █████                                          ███            █  
#   █   █                                        █   █           █  
#   █   █   ████    ███   ███ ██    ███   ████   █       ███   █████
#   █   █       █  █   █  █  █  █  █   █  █   █   ███   █   █    █  
#   █   █   █████  █████  █  █  █  █   █  █   █      █  █████    █  
#   █   █  █    █  █      █  █  █  █   █  █   █  █   █  █        █  
#  █████    ███ █   ███   █  █  █   ███   █   █   ███    ███      ██
#--------------------------------------------------------------------------------------------------
DaemonSet - Tworzą się same, gdy powoływany jest do rzycia nowy nod. Kasuje się sam, gdy nod przestaje istnieć.
Film 70 Certified Kubernetes Administrator (CKA) with Practice Tests
# Uruchamiane automatycznie przez Kube-API server (DaemonSet Controller):


definicja pliku, wygląda tak samo jak ReplicaSet
┌───────────────────
apiversion: apps/vl
kind: DaemonSet
metadata:
  name: monitoring-daemon
spec:
 selector:
    matchLabels:
       app: monitoring-agent
 template:
    metadata:
       labels:
         app: monitoring-agent
     spec:
       containers:
        -name: monitoring-agent
         image: moniLoring-agenl
└───────────────────

kubectl create -f nazwaPliku.yaml

#--------------------------------------------------------------------------------------------------                                          
Statyczne Pody - czyli takie, które zawsze są dostępne (zawsze wuruchominone) jak kontenery systemowe. 
# Filmin 6_7 Kurs Kubernetes od podstaw - zarządzanie i automatyzacja kontenerów. Statyczne Pody
# Uruchamiane automatycznie przez kubelet z folderu:
/etc/kubernetes/manifests
# ścieżke mozna podejżeć lub zmienić w pliku /var/lib/kubelet/config.yaml


#--------------------------------------------------------------------------------------------------                                          
   #####                    #                                                    #  
    #   #                   #                                                    #
    #   #    ###    ####    #        ###    #    #   ### ##     ###    ####    #####
    #   #   #   #   #   #   #       #   #   #   #    #  #  #   #   #   #   #     #
    #   #   #####   #   #   #       #   #    # #     #  #  #   #####   #   #     #
    #   #   #       ####    #   #   #   #     #      #  #  #   #       #   #     #
   #####     ###    #        ###     ###     #       #  #  #    ###    #   #      ##
                    #                       #
#--------------------------------------------------------------------------------------------------                                          
Deployment - "spina" w całosc nasza aplikację (troszkę jak docker-compose). 
Możemy za jego pomoca skalować, podnosić wersję, cofac się.
Stworzenie deployment - automatycznie stworzy ReplicaSet
╔═════════════════════════╗ 
║  Deployment             ║ 
║  ┌───────────────────┐  ║ 
║  │ ReplicaSet        │  ║ 
║  │ ┌─────────────┐   │  ║ 
║  │ │  POD        │   │  ║ 
║  │ │ ┌─────────┐ │   │  ║ 
║  │ │ │Kontener │ │   │  ║ 
║  │ │ │ <app>   │ │   │  ║ 
║  │ │ └─────────┘ │   │  ║ 
║  │ └─────────────┘   │  ║ 
║  └───────────────────┘  ║ 
╚═════════════════════════╝ 

┌───────────────────
apiVersion: apps/v1          
kind: Deployment             
metadata:                   
  #namespace: webapp-namespace
  name: deployment-moja-nazwa       
  labels:                   
    app: k8smaestro-webapp  
spec:                                  # specyfikacja dla deploymentu
  replicas: 3  
  template:                 
    metadata:
      labels:
        app: k8smaestro-webapp         # na podstawie tych etykiet, wiadomo ktore pody naleza do jakich deploymentow
        #name: app-2048
    spec:                              # specyfikacja dla poda
      containers:
      - name: webapp                   # obraz bazowy
        image: k8smaestro/web-app:1.0
        #- image: k8smaestro/2048
        #imagePullPolicy: Always       # polityka pobierania obazu na klaster. Always - zawsze pobiera obraz
        #name: app-2048
        #ports:                        # dodana informacja o porcie, gdy konfigurujemy Services
        #- containerPort: 80
  selector:  
    matchLabels:
      app: k8smaestro-webapp           # na podstawie tych etykiet, wiadomo ktore pody naleza do jakich deploymentow
└───────────────────        

# Usage:
  kubectl create deployment NAME -image=image [--dry-run] [options]

# stworzenie/uruchomienie nowego deploymentu
kubectl create -f deployment-def.yml


# edycja działajacego deploymentu:
kubectl set image deployment/nazwa-deploymentu ubuntu=ubuntu:20.04
kubectl set env deployment/nazwa-deploymentu -c ubuntu SLEEP_TIME=110


# kasowanie
kubectl delete -f deployment-def.yml
kubectl delete nazwa-deploymentu  

# kasowanie wszystkiego
kubectl delete deployment --all

#--------------------------------------------------------------------------------------------------
# ████        █     █                   █                         █     █     █
# █   █       █     █                   █                         █     █     █                    █               █
# █   █  ███  █     █      ███  █   █ █████      ████  ████   █████     █     █  ███  █ ███  ███       ███  ████      ████   ████
# ████  █   █ █     █     █   █ █   █   █            █ █   █ █    █     █     █ █   █ ██    █     ██  █   █ █   █ ██  █   █ █   █
# █ █   █   █ █     █     █   █ █   █   █        █████ █   █ █    █      █   █  █████ █      ███   █  █   █ █   █  █  █   █ █   █
# █  █  █   █ █   █ █   █ █   █ █   █   █       █    █ █   █ █    █       █ █   █     █         █  █  █   █ █   █  █  █   █  ████
# █   █  ███   ███   ███   ███   ████    ██      ███ █ █   █  █████        █     ███  █      ███  ███  ███  █   █ ███ █   █     █
#                                                                                                                           ████ 
#--------------------------------------------------------------------------------------------------
# Rollout and Versioning
# skalowanie

# zakładam, ze zmieniam wersje kontenera, np spec.template.spec.containers.name[].image: nginx:1.7.1
# aktualizuje kontener (deployment) poleceniem:
kubectl apply -f deployment-definiton.yml        
# sa 2 strategie podnoszenia wersji: Recreate (ubija wszsytkie i stawia wszystkie) i RollingUpdate (ubija i stawia po jednym, nie ma widocznej przerwy). Nie wiem jak to ustawić.


kubectl scale deployment voting-app-deploy --replicas=2

kubectl set image deployment nazwa-deploymentu httpd=httpd:2.4 --record=true  # przykładowa zmiana wersji kontenera podczas działania 
kubectl rollout undo deployment nazwa-deploymentu   # cofnięcie do porzedniej wersji (przed set image). wydaje mi się, że wcześniej trzeba dodać --record=true
kubectl rollout undo deployment/myapp-deployment


kubectl rollout status deployment/myapp-deployment
kubectl rollout status deployment.app/myapp-deployment
kubectl rollout history deployment/myapp-deployment
kubectl rollout history deployment myapp-deployment  # na sterfie kursów, zapis rozdzielony spacją

# Aby zmienić wersję, trzeba wskazac na nowy kontener w pliku w:
spec.template.spec.containers.image: k8smaestro/web-app:1.1


kubectl apply -f deployment-definiton.yml        
# drugi, niezalecany sposób:
kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1 --record=true
#                                                                 └tylko, gdy chcemy, zeby się zapisywała i odczytywała hostoria poprzez rollout



#--------------------------------------------------------------------------------------------------
   #   #             #                               #
   ##  #             #                               #       #
   ##  #    ###    #####   #     #    ###    # ###   #  #         ####     ####
   # # #   #   #     #     #     #   #   #   ##      # #    ##    #   #   #   #
   #  ##   #####     #     #  #  #   #   #   #       ##      #    #   #   #   #
   #  ##   #         #     # # # #   #   #   #       # #     #    #   #    ####
   #   #    ###       ##    #   #     ###    #       #  #   ###   #   #       #
                                                                          ####
#--------------------------------------------------------------------------------------------------
Networking
CNI Container Networking Interface
# film 37 (Kubernetes for the Absolute Beginners - Hands-on)
# Kazdy pod otrzymuje swój adres IP: 10.244.0.2, 10.244.0.3 itd
# Wszsytkie pody na naszym lokalnym kompie, widziane są przez chmórę: 10.244.0.0

# Narzędzia do konfiguracji sieci:
Cisco, cilium, flannel Vilmer, vmware NSX, Psyllium, Big Cloud fabric, Kalikow

# jakaś konfiguracja sieci dla kubernetsa
ps -aux | grep kubelet

# jakies narzędzie do zarządzania sieciami
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d'\n')"
# teraz po wywołaniu kubectl get pods -n kube-sytem    powinny pojawić sie kilka podów "weave-net-5gcm"
kubectl logs weave-net-5gcm weave -n kube-system

# żeby podejżeć IP przydzielone do naszego servisu
iptables -L -t nat | grep my-color-service


DNS
Kube DNS zawiera:
  Hostname:  web-service
  Namespace: apps
  Type: svc
  Root: slucter.local
  IP Address: 10.107.37.188

# Aby się odwołać, trzeba wpisać;
curl http://web-service.apps.svc.cluster.local


Ingress
# Gdy chce spiąc na jednej stronie wear-service z video-service   Film 98 (Kubernetes Certified Application Developer (CKAD) with Tests)
# ingeres definicja ścieżkię od urzytkownika, do odpowiedniego poda
Egress - definicja ścieżki powrotu informacji
# Przykładowo, możemy wysyłać pytania przez podrt 80 a otrzymywac odpowiedzi z portu 5000

INGRESS CONTROLLER

Deployment
┌───────────────────
apiVersion: extensions/v1beta1   # w innym filmie tutaj było  apps/v1
kind: Deployment             
metadata:                   
  name: nginx-ingress-controller       
  labels:                   
    app: k8smaestro-webapp  
spec:                       
  replicas: 1
  selector:  
    matchLabels:
      name: nginx-ingress
  
  template:                 
    metadata:
      labels:
        name: nginx-ingress
    spec:
      containers:
        - name: nginx-ingress-controller
          image: quqy.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
      args:
        - /nginx-ingress-controller
        - --configmap=$(POD_NAMESPACE)/nginx-configuration
      env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
      
      ports:
        - name: http
          containerPort: 80
        - name: https
          containerPort: 442
└───────────────────

ConfigMap
┌───────────────────
kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration

└───────────────────

Service
┌───────────────────
apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
    protocol: TCP
    name: http
  - port: 443
    targetPort: 443
    protocol: TCP
    name: https
  selector:
    name: nginx-ingress

└───────────────────

Auth
┌───────────────────
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
└───────────────────


#-----------------------------------------------------------
Ingress-wear.yaml            # gdy korzystamy tylko z jednego typu wysyłancyh danchy (tylko tekst)
┌───────────────────
apiVersion: extensions/v1beta1
kind: Ingress
metadata:                   
  name: ingress-wear
spec:        
  backend:
    serviceName: wear-service
    servicePort: 80
└───────────────────
kubectl create -f Ingress-wear.yaml 

Ingress-wear-watch.yaml  # Rule 1      gdy chcemy osiągnąc www.mystore.com/WEAR i www.mystore.com/WATCH
┌───────────────────
apiVersion: extensions/v1beta1
kind: Ingress
metadata:                   
  name: ingress-wear-watch
spec:        
  rules:
  - http:
    - path: /wear
      backend: 
        serviceName: wear-service
        servicePort: 80
    - path: /watch
      backend: 
        serviceName: watch-service
        servicePort: 80
└───────────────────


Ingress-wear-watch.yaml  # Rule 2      gdy chcemy osiągnąc www.WEAR.mystore.com i www.WATCH.mystore.com
┌───────────────────
apiVersion: extensions/v1beta1
kind: Ingress
metadata:                   
  name: ingress-wear-watch
spec:        
  rules:
  - host: wear.my-online-store.com
    http:
      paths: 
      - backend: 
          serviceName: wear-service
          servicePort: 80
  - host: watch.my-online-store.com
    http:
      paths:
      - backend: 
          serviceName: watch-service
          servicePort: 80
└───────────────────

#--------------------------------------------------------------------------------------------------
Network Policy  - zwiazane z Ingres i Egres
# definiuje takiego firewala, okresla port wejści i port wyjscia
# Film 105  Kubernetes Certified Application Developer (CKAD) with Tests

policy-definition.yaml
┌───────────────────
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:                   
  name: db-policy
spec:        
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          name: api-pod
      #  jakies wyjatki i wiecej opcji w filmie # Film 106  Kubernetes Certified Application Developer (CKAD) with Tests    
    ports:
    - protocol: TCP
      port: 3306
└───────────────────



#--------------------------------------------------------------------------------------------------
    ###
   #   #                              #
   #        ###    # ###   #     #          ###     ###     ###
    ###    #   #   ##      #     #   ##    #   #   #   #   #
       #   #####   #        #   #     #    #       #####    ###
   #   #   #       #         # #      #    #   #   #           #
    ###     ###    #          #      ###    ###     ###     ###
#--------------------------------------------------------------------------------------------------
Services
# film 38 (Kubernetes for the Absolute Beginners - Hands-on)
# Usługi umożliwiają komunikację między róznymi komponentami w ramach aplikacji i po za nią
# My łączymy się za pomocą adrestu i portu, a Service, ten port przełaczy z IP konkretnego Poda

Service Types:
 - ClusterIP (domyślnie) - Wystawia serwis poprzez wewnętrzny adres IP w klastrze. W ten sposób serwis jest dostępny tylko wewnątrz klastra.
 - NodePort - Wystawia serwis na tym samym porcie na każdym z wybranych węzłów klastra przy pomocy NAT. W ten sposób serwis jest dostępny z zewnątrz klastra poprzez <NodeIP>:<NodePort>. Nadzbiór ClusterIP.
 - LoadBalancer - Tworzy zewnętrzny load balancer u bieżącego dostawcy usług chmurowych (o ile jest taka możliwość) i przypisuje serwisowi stały, zewnętrzny adres IP. Nadzbiór NodePort.
 - ExternalName - Przypisuje Service do externalName (np. foo.bar.example.com), zwracając rekord CNAME wraz z zawartością. W tym przypadku nie jest wykorzystywany proces przekierowania ruchu metodą proxy. Ta metoda wymaga kube-dns w wersji v1.7 lub wyższej lub CoreDNS w wersji 0.0.8 lub wyższej.


#--------------------------------------
NodePort
#                                     ╔════════════════════════════════════════╗ 
#                                     ║           192.168.1.2                  ║
# mój laptop                          ║─────┐    ┌────────┬──┐    ┌──┬───────┐ ║
# curl http://192.168.1.2:30008 ────> ║30008│ ──>│Service │80│ ──>│80│    POD│ ║
#   Zakres: 30000 - 32767             ║─────┘    │        └──┤    ├──┘       │ ║
#                                     ║          └───────────┘    │10.244.0.2│ ║
#                                     ║ Node                      └──────────┘ ║
#                                     ╚════════════════════════════════════════╝ 
#                                     nodePort            port     targetPort
┌───────────────────
apiVersion: v1          
kind: Service
metadata:                   
  name: myapp-service     # dowolna nazwa
  #labels:                #─┬─ dane takie jak w "Deployment" w metadata.labels
    #name:                # │  żeby nasz serwis wiedział, do jakiego kontenera się przypiąć
    #app:                 #─┘
spec:
  #selector:              # selector, to taki filtr do "metadata.labels" w podach którymi ma się zająć
  #  app: App1

  ┌─────────── #opcja 1 - ClusterIP (domyślnie)
  type: ClusterIP
  ports:
  - targetPort: 80
    port: 80
  └───────────
  ┌─────────── #opcja 2 - NodePort
  type: NodePort
  ports:
  - targetPort: 80
    port: 80
    nodePort: 30008
  └───────────
  ┌─────────── #opcja 3 - LoadBalancer
  type: LoadBalancer
  ports:
  - targetPort: 80
    port: 80
    nodePort: 30008
  └───────────
  ┌─────────── #opcja 4 - przykład serwisu ze Strefy Kursów. koles nie podaje .type, tylko .protocol
  ports:
  - port: 80
    protocol: TCP             # jakiego typu protokół
  └───────────

  selector:  
    app: myapp       #─┬─ dane takie, jak w "Pod" w sekcji metadata.labels
    type: front-end  #─┘ 
└───────────────────

#--------------------------------------


# ustawienie portu do nasluchu:
kubectl expose deployment strefakursow-nginx --type=NodePort --port=8080

kubectl expose deployment deployment-2048 -n game

#--------------------------------------
# tworzenie servisu
kubectl creatae -f service-definition.yml
kubectl apply -f service-definition.yml

# pobireanie informacji o uruchomionych serwisach
kubectl get services

# pobranie adresu url w minikube (film 39)
minikube service myapp-service --url

# jeżeli istnieje serwis, powinno udac sie pobrac strone (nie wiem skąd ten adres)
curl http://192.168.1.2:30008


# W przykładzie, w Deployment, trzeba dopisać:
spec.template.spec.containers:
                   - name: ...
                     ports:               # dodana informacja o porcie
                     - containerPort: 80

#--------------------------------------------------------------------------------------------------
#    #   #                  #
#    ##  #                  #
#    ##  #     ###      #####     ###
#    # # #    #   #    #    #    #   #
#    #  ##    #   #    #    #    #####
#    #  ##    #   #    #    #    #
#    #   #     ###      #####     ###
#--------------------------------------------------------------------------------------------------
Node

kubectl get nodes
kubectl drain node-1      # powoduje zwolnienie wskazanego noda i przeniesienie poda do innego noda
kubectl cordon node-2
kubectl uncordon node-1

# programy do aktualizowania:
kube-apiserver
controller-manager
kube-scheduler
kubelet
kube-proxy
kubectl
ETCD CLUSTER
CoreDNS

References
 https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md
 https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/api-group.md
 https://blog.risingstack.com/the-history-of-kubernetes/
 https://kubernetes.io/docs/setup/version-skew-policy/

 Cluster Upgrade Process

 Zalecany jest upgrade co jedną wersję z 1.08.xx na 1.09.xx
kubeadm upgrade plan       # wyswietli informacje o dostępnych wersjach
kubeadm upgrade apply

apt-get upgrade -y kubeadm=1.12.0-00        # (M) operacje na master
kubeadm upgrade apply v1.12.0               # (M)
apt-get upgrade -y kubelet=1.12.0-00        # (M)
systemctl restart kubelet                   # (M) po tym poleceniu, powinna być aktualna wersja Mastera
kubectl drain node-1                        # (M) przeniesienie podów do innego "worker noda"
apt-get upgrade -y kubeadm=1.12.0-00        # (n1) operacje robione na nodzie-1
apt-get upgrade -y kubelet=1.12.0-00        # (n1)
kubeadm upgrade node config --kubelet-version v1.12.0 # (n1)
systemctl restart kubelet                   # (n1)
kubectl uncordon node-1                     # (M)
kubectl drain node-2                        # (M)
kubectl uncordon node-2                     # (M)
kubectl drain node-3                        # (M)


Przykład upgrade. Film 124  Certified Kubernetes Administrator (CKA) with Practice Tests
Instrukcja: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/

Backup Film 128  Certified Kubernetes Administrator (CKA) with Practice Tests

#--------------------------------------------------------------------------------------------------
   #   #
   ##  #
   ##  #    ####    ### ##     ###     ###    ####     ####     ###     ###
   # # #        #   #  #  #   #   #   #       #   #        #   #   #   #   #
   #  ##    #####   #  #  #   #####    ###    #   #    #####   #       #####
   #  ##   #    #   #  #  #   #           #   ####    #    #   #   #   #
   #   #    ### #   #  #  #    ###     ###    #        ### #    ###     ###
                                              #
#--------------------------------------------------------------------------------------------------
Namespace - taki "wirtualny klaster" w ramach tego samego fizycznego klastra
Rodzaje namespace: Pod, ReplicaSet, Deployment, Service, itp...
Domyślnie trafia on do namestace default
Gdy tworzony jest Service, twory sie wpis w DNS:
<service-name>.<namespace-name>.svc.cluster.local


┌───────────────────
apiVersion: v1          
kind: Namespace
metadata:                   
  name: webapp-namespace  # dowolna nazwa
└───────────────────

kubectl get ┬ namespaces                   # lista przestrzeni nazw
            ├ ns                           # skrócony zapis
            ├ all -n webapp-namespace      # wszystkie elementy przypisane do danej przestrzeni nazw
            ├ pods -n webapp-namespace     # jakie pody naleza do tej przestrzeni nazw
            └ pods --all-namespaces        # wszsytkie pody ze wszsytkich przestrzeni

kubectl apply -f nazwaPliku.yaml           # zwykłe uruchomienie dla namespace default
kubectl apply -f nazwaPliku.yaml -n nazwaNamespace    # uruchomienie w podanej przestrzeni nazw

kubectl create namespace nazwaPrzestrzeni  # ręczne tworzenie 

# trwałe przełączenie się na inną przesteń nazw
kubectl config set-context $(kubectl config current-context) --namespace=game

# kasowanie namespace
kubectl delete ns nazwaNamespace

#--------------------------------------------------------------------------------------------------
Resorce Quota - chyba taka przestrzeń nazw z ograniczeniami

Compute-quota.yaml
┌───────────────────
apiVersion: v1
kind: ResourccQuota
metadata:
    name: compute-quota
    namespace: dev
spec:
  hard:
    pods: "10"
    requests.cpu: "4"
    requests.memory: 5Gi
    limits.cpu: "10"
    limits.memory: 10Gi
└───────────────────

>kubectl create -f compute-quota.yaml


#--------------------------------------------------------------------------------------------------
    #####             #    
        #             #
        #     ###     ####
        #    #   #    #   #
        #    #   #    #   #
    #   #    #   #    #   #
     ###      ###     ####
#--------------------------------------------------------------------------------------------------
JOB - taki pod, uruchomiony raz. Nie bedzie ponownie uruchamiany.

# przykład kontenera, który się odpali, policzy, zwróci 5 i (chyba) pozostanie nie aktywny
docker run ubuntu expr 3 + 2

# Prosyt job, który ma policzyć 3 +2:
# job-math-add.yaml
┌───────────────────
apiVersion: batch/v1
kind: Job
metadata: 
  name: math-add-job
spec:
  # ttlSecondsAfterFinished: 10   # [opcjonalnie*] po 10 sekundach, powinien się automatycznie usunąć
  completion: 3                   # [opcja] ile sztuk ma sie uruchomić prawidłowo
  parallelism: 3                  # [opcja] (chyba) uruchomi tylko 3, nawet jeśli uruchomią się błędnie
  template:
    spec:
      containers:
      - name: math-add
        image: ubuntu
        command: ['expr', '3', '+', '2']
      restartPolicy: Never    
  #  metadata:
  #  name: odliczanie
└───────────────────
# uruchomienie Joba
kubectl create -f job-math-add.yaml
# sprawdzam jaki powstal pod  (albo get job, ale nie zobaczymy hashtagu)
kubectl get pods
# zobaczenie wyniku:
kubectl logs math-add-job-hsm9h
# usunięcie już nie potrzbengo joba
kubectl delete job math-add-job



# Prosyt job, liczący od 10 w dóły:
┌───────────────────
apiVersion: batch/v1
kind: Job
metadata: 
  name: strefakursow-odliczanie
spec:
  ttlSecondsAfterFinished: 10  # [opcjonalnie*] po 10 sekundach, powinien się automatycznie usunąć
  template:
    metadata:
      name: odliczanie
    spec:
      containers:
      - name: container-odliczanie
        image: centos:7
        command:
        - "bin/bash"
        - "-c"
        - "for i in 9 8 7 6 5 4 3 2 1 ; do echo $i ; done"
      restartPolicy: Never    
└───────────────────
# uruchomienie:
kubectla apply -f ./nazwa-pliku.yaml

# Aby działało autowyłącznie, trzeba je aktywować poleceniem:
minikube --feature-gates="TTLAfterFinished=true"


# Job, który ma dla nas pobrac coś z YT:
┌───────────────────
apiVersion: batch/v1
kind: Job
metadata:
  name: strefakursow-yt-pobranie
spec:
  template:
    metadata:
      name: yt-pobranie
    spec:
      restartPolicy: Never
      containers:
      - name: yt-download
        image: wernight/youtube-dl
        # pierwszy sposob
        #command: ["youtube-dl", "adres_ktory_chcemy-pobrac_https://www.youtube.com/watch?v=dkfjdkjfk"]
        # Drugi sposob. Wywołana aktualizacja wtyczki
        command:
        - "/bin/bash"
        - "-c"
        - "pip install -U youtube-dl; youtube-dl https:/youtube.com/watch?v=dfjkfjdk"


apiVersion: batch/v1
kind: Job
metadata: 
  name: strefakursow-yt-pobranie
spec:
  template:
    metadata:
      name: yt-pobranie
    spec:
      restartPolicy: Never        
      containers:
      - name: yt-download
        image: wernight/youtube-dl
        # pierwszy sposob
        #command: ["youtube-dl", "adres_ktory_chcemy-pobrac_https://www.youtube.com/watch?v=dkfjdkjfk"]
        # Drugi sposob. Wywołana aktualizacja wtyczki
        # command:
        # - "/bin/bash"
        # - "-c"
        # - "pip install -U youtube-dl; youtube-dl https:/youtube.com/watch?v=dfjkfjdk"
        # Trzeci sposób:
        command:
        - "/bin/sh"
        - "-c"
        - "youtube-dl -n --cookies /cookies.txt https://youtu.be/zAPjjx05Y3k"
└───────────────────




#    ███                            █████           █    
#   █   █                               █           █
#   █       █ ███    ███    ████        █    ███    ████
#   █       ██      █   █   █   █       █   █   █   █   █
#   █       █       █   █   █   █       █   █   █   █   █
#   █   █   █       █   █   █   █   █   █   █   █   █   █
#    ███    █        ███    █   █    ███     ███    ████


# Job, który ma się wykonać o określonej porze
┌───────────────────
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: strefakursow-cron
spec:
  schedule: "*/1 * * * *"
          #  │   │ │ │ └ day of the week (0 - 6) (Sunday to Saturday;  7 is also Sunday on some system)
          #  │   │ │ └ month (1 - 12)
          #  │   │ └ hour (0 - 59)
          #  │   └ hour (0 - 59)
          #  └ minute (0 - 59)  1 -w pierwszej minucie;  */1 -co minutę  wiecej na https://crontab.guru
  jobTemplate:
    spec:
      completion: 3                    # [opcja] ile sztuk ma sie uruchomić prawidłowo
      parallelism: 3                   # [opcja] (chyba) uruchomi tylko 3, nawet jeśli uruchomią się błędnie
      template:
        spec:
          restartPolicy: Never
          containers:
          - name: strefakursow-pogoda-cron
            image: ubuntu:18.04
            command:
            - "/bin/bash"
            - "-c"
            - "apt-get update > /dev/null; apt-get install curl -y > /dev/null; curl -s http://wttr.in/Radom"
#                                                                └ od razu potwierdz  └ silent dla error messages
└───────────────────

# uruchomienie:
kubectl create -f nazwaPliku.yaml

# podgląd logów (oczywiście trzeba zdobyć id aktualnie uruchomionego poda)
kubectl logs job.batch/strefakursow-cron-27516178

# kasowanie:
kubectl delete cronjob strefakursow-cron


#--------------------------------------------------------------------------------------------------
#                  █
#                  █
#  █     █   ███   █      █   █  ███ ██    ███    ███
#  █     █  █   █  █      █   █  █  █  █  █   █  █
#   █   █   █   █  █      █   █  █  █  █  █████   ███
#    █ █    █   █  █   █  █   █  █  █  █  █          █
#     █      ███    ███    ████  █  █  █   ███    ███
#--------------------------------------------------------------------------------------------------
volumes

# Storage drivers, zaleznie od wersji linuxa?:
AUFS | ZFS | BTRFS | Device Mapper | Overlay | Overlay2

# Volume Drivers, chyba zaleznie od usługi chmury
Local | Azure File Storage | Convoy | DigitalOcean Block Storage | Flocker | gce-docker | GlusterFS | NetApp | RexRay | Portworx | VMware vSphere Storage

https://docs.docker.com/engine/extend/legacy_plugins/


# przykład z filmu 173 Certified Kubernetes Administrator (CKA) with Practice Tests
┌───────────────────
apiVersion: v1
kind: Pod
metadata:
  name: random-number-generator
spec:
  containers:
  - name: alpine
    image: alpine
    command: ["/bin/sh","-c"]
    args: ["shuf -i 0-100 -n 1 >> /opt/number.out;"]  # losowanie liczby 0 - 100
    volumeMounts:
    - name: data-volume 
      mountPath: /opt             # folder w kontenerze

  volumes:
  - name: data-volume
    ┌───────────────────          # w nodzie (nie zalecane gdy korzytamy z kilku nodów)
    hostPath:
      path: /data                 # folder w nodzie (po za kontenerem)
      type: Directory             # Directory -musi istnieć.   DirectoryOrCreate -jeśli nie ma, to go stworzy. Więcej o typach: https://kubernetes.io/docs/concepts/storage/volumes/
    └───────────────────
    ┌───────────────────          # w zewnetrznej usłudze, na przykładzie aws
    aswElasticBlockStorage:
      volumeID: <volume-id>
      fsType: ext4
    └───────────────────
    ┌───────────────────          # PVs Persistent Volume Claim - wymaga zdefinowania PersistentVolumeClaim  Film 110 Kubernetes Certified Application Developer (CKAD) with Tests
    persistentVolumeClaim:
      claimName: myclaim
    └───────────────────
└───────────────────

#--------------------------------------------------------------------------------------------------
# Persistent Volume (Static Provisioning)
# przed stworzeniem pv - musze stworzyć na chmurze/dysku miejsce  gcolud beta disks create --size 1GB --region us-east1 pb-disk
pv-definition.yaml
┌───────────────────
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-vol1
spec:
  accessModes:          # ReadOnlyMany | ReadWriteOnce | ReadWriteMany
    - ReadWriteOnce
  capacity:
    storage: 1Gi
  ┌───────────────────  
  hostPath:
    path: /tmp/data     # nie urzywać tego w wersji produkcyjnej
  └───────────────────
  ┌───────────────────
  aswElasticBlockStorage:
    volumeID: <volume-id>
    fsType: ext4
  └───────────────────
  ┌───────────────────  
  gcePersistentDisk:
    fsType: ext4  
  └───────────────────

#  persistentVolumeReclaimPolicy: Retain  # Retain | Delete | Recycle   #okresla, czy możliwe jest skasowanie volumenu
└───────────────────

kubectl create -f pv-definition.yaml

kubectl get persistentvolume

#--------------------------------------------------------------------------------------------------
# Persistent Volume Claim 

# Przykład poda korzytajacego z persistentVolClaim: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes

pvc-definition.yaml
┌───────────────────
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:          # ReadOnlyMany | ReadWriteOnce | ReadWriteMany
    - ReadWriteOnce
  #storageClassName: google-storage         # *1v   wymaga zdefinowania StorageClass
  resources:
    requests:
      storage: 500Mi
└───────────────────

kubectl create -f pvc-definition.yaml
kubectl get persistentvolumeclaim

kubectl delete persistentvolumeclaim myclaim



#--------------------------------------------------------------------------------------------------
# Storage Class  (Dynamic Provisioning)

sc-definition.yaml
┌───────────────────
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: google-storage                      # *1^
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard                    # pd-standard | pd-ssd
  replication-type: none               # none | regional-pd
└───────────────────




#--------------------------------------------------------------------------------------------------
# Przykład z filmu [5:00] 5_4 Zmienne środowiskowe i wolumeny w podach (Kurs Kubernetes od podstaw - zarządzanie i automatyzacja kontenerów )
# 
┌───────────────────
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-strefakursow-env
spec:
  containers:
  - name: ubuntu-once
    image: ubuntu:18.04
    env:
    - name: LINK_MY
      value: "https://strefakursow.pl"
    - name: LICZBA_KURSOW
      value: "20"
    volumeMounts: 
    - mountPath: /mntvol          # informacja, w jakim miejscu ma zostać zamontowany
      name: volume-no1            # nazwa musi być taka sama jak w 'volumes'
    command:
    - "/bin/bash"
    - "-c"
    - "env; echo  ; for (( i=1 ; $i<=$LICZBA_KURSOW ; i++ )); do echo 'To jest pozycja numer: ' $i >> /mntvol/logs.test; done "
    # to co wypisze ten kontener, ma zapisać w pliku /mntvol/logs.test
    # następnie, ten plik jest podpięty do volumenu
    # aby kontener nie wyłaczyl się, możemy dodać do linni komend: sleep inf

    # drugi kontener
  - name: ubuntu-work
    image: ubuntu:18.04
    volumeMounts:
    - mountPath: /strefakursow
      name: volume-no1
    command:
    - "/bin/bash"
    - "-c"
    - "sleep inf"

  restartPolicy: Never
  volumes:                        #     volumen
  - name: volume-no1              #
    hostPath:                     # Jest kilka typów wolumenów. Tutaj skorzystamy z lokalnego storage na naszym danym nodzie
      path: /srv/persistent/
      type: DirectoryOrCreate     # Directory -musi istnieć.   DirectoryOrCreate -jeśli nie ma, to go stworzy. Więcej o typach: https://kubernetes.io/docs/concepts/storage/volumes/
    #configMap:                   # Odczyt z poda configMap
    #  name: app-config    
└───────────────────

# UWAGA! Przed uruchominieniem tego pliku, trzeba dla volume stworzyć folder /srv/persistent/
# Tworzenie ręcznie:
# 1. Wejdz na minikube
minikube ssh
# 2. stwórz folder
sudo mkdir -p /stv/persistent/
# 3. wyjdz z minikube
exit

#--------------------------------------------------------------------------------------------------
Stateful Sets 
# Definiuje sie podobnie jak deployment
# UWAGA! Na OVH nie zadziałało

statefulset-definition.yaml
┌───────────────────
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  template:
    metadata:
      labels:
        app: mysql
    spec: 
      containers:
      - name: mysql
        image: mysql
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  serviceName: mysql-h
  podManagementPolicy: Parallel
└───────────────────







#--------------------------------------------------------------------------------------------------
#    ███                      ███                 █     █
#   █   █                    █       █            ██   ██
#   █        ███    ████     █             ████   █ █ █ █    ████    ████     ███ 
#   █       █   █   █   █   ████    ██    █   █   █  █  █        █   █   █   █    
#   █       █   █   █   █    █       █    █   █   █     █    █████   █   █    ███ 
#   █   █   █   █   █   █    █       █     ████   █     █   █    █   ████        █
#    ███     ███    █   █    █      ███       █   █     █    ███ █   █        ███ 
#                                         ████                       █
#--------------------------------------------------------------------------------------------------

┌───────────────────
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config                     # *1v
data:
  APP_COLOR: blue
  APP_MODE: prod
  port: 3306 
└───────────────────

# Zastosowanie ConfigMaps w Pod:
# Wersja "tradycyjna" (bez ConfigMap)
spec.containers[].env:   
                  - name: APP_COLOR
                    value: blue

# Dla odczytu jednej wartości z ConfigMap:
spec.containers[].env:
                  - name: APP_COLOR
                    valueFrom:
                      configMapKeyRef:
                        name: app-config
                        key: APP_COLOR

# Dla odczytu wszystkich kluczy z ConfigMap:
spec.containers[].envFrom:  # !zamiast env jest envFrom:
                  - configMapRef:   
                      name: app-config 

volumes:
- name: app-config-volume
  configMap:
    name: app-config


#--------------------------------------------------------------------------------------------------
#   ███                                 █  
#  █   █                                █
#  █       ███    ███   █ ███   ███   █████
#   ███   █   █  █   █  ██     █   █    █
#      █  █████  █      █      █████    █
#  █   █  █      █   █  █      █        █
#   ███    ███    ███   █       ███      ██
#--------------------------------------------------------------------------------------------------
#  pody, które generują jakieś hasła
┌───────────────────
apiVersion: v1
kind: Secret
metadata:
  name: strefa-secret                  # *1v
#type: kubernetes.io/basic-auth # wymusi dwa pola username i password
#type: Opaque
data:
  user: "cm9vdA=="            # echo -n "root" | base64
  pass: "VGFqbmVIYXNsbw=="             # *2v
  DB_Host: bXlzcWw=           # echo -n "mysql" | base64
  DB_User: cm9vdA==           # echo -n "root" | base64
  DB_Password: cGFzd3Jk       # echo -n "paswrd" | base64
stringData:                   # sekcja jawnych danych (nie szyfrowane)
  jawne: "Dane nie zaszyfrowane"
  ip: "192.168.2.1"
└───────────────────

# generowanie szyfrowanych danych w linuxie
echo -n "paswrd" | base64               #=> cGFzd3Jk
#odszyfrowywanie
echo -n "cGFzd3Jk" | base64 --decode    #=> paswrd

# Uruchomienie 
kubectl apply -f nazwaPliku.yaml

# podgląd
kubectl get secrets

# podgląd wszystkich
kubectl get secrets --all-namespace

# podglad naszego secret
kubectl get secret nazwaNaszegoSecreta -o yaml
kubectl describe secret nazwaNaszegoSecreta
# bedzie tu informacja podana w bajtach

# kasowanie
kubectl delete secret nazwaNaszegoSecreta

# Zastosowanie Secret w Pod:
# Dla odczytu jednej wartości z ConfigMap:
spec.containers[].envFrom:
                  - name: DB_Password
                    valueFrom:
                      secretKeyRef:
                        name: app-secret
                        key: DB_Password

# Dla odczytu wszystkich kluczy z ConfigMap:
spec.containers[].envFrom:  # !zamiast env jest envFrom:
                  - configMapRef:   
                      name: app-config 

volumes:
- name: app-secret-volume
  secret:
    secretName: app-secret


# przykładowy Pod, korzsytający z naszych secretów
┌───────────────────
apiVersion: v1
kind: Pod
metadata:
  name: database-with-volume
spec:
  containers:
  - name: strefakursow-db-vol
    image: mysql
    volumeMounts:
    - name: dbconnection               # *3v
      mountPath: "/root/secrets"
      readOnly: true                   # ustawienie tylko odczytu
    env:
      - name: MYSQL_ROOT_PASSWORD
        valueFrom:
          secretKeyRef:
            name: strefa-secret        # *1^
            key: pass                  # *2^
  volumes:
  - name: dbconnection                 # *3^
    secret:
      secretName: strefa-secret
└───────────────────


# podejżenie plików secret w podach:
kubectl exec -it nazwaPoda ls /var/run/secrets/kubernetes.io/serviceacount
kubectl exec -it nazwaPoda cat /var/run/secrets/kubernetes.io/serviceacount/token


#--------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------
Kopiowanie plików
┌───────────────────
apiVersion: apps/v1
kind: Deployment
metadata:
  name: http-strefakursow
  labels:
    app: httpd-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: http-app
  template:
    metadata:
      labels:
        app: http-app
    spec:
      containers:
      - name: http
        image: httpd
        ports:
        - containerPort: 80
└───────────────────

# kopiowanie pliku:
kubectl cp index.html http-strefakursow-745758289-59mlr:/usr/local/apache2/htdocs/

# kopiowanie katalogu:
kubectl cp ../5_7 http-strefakursow-745758289-59mlr:/usr/local/apache2/htdocs/



#--------------------------------------------------------------------------------------------------
#  █████                █
#  █                    █               █       
#  █      █   █  ████   █  █   ███          ███ 
#  ████   █   █  █   █  █ █   █   █    ██  █   █
#  █      █   █  █   █  ██    █         █  █████
#  █      █   █  █   █  █ █   █   █     █  █    
#  █       ████  █   █  █  █   ███   █  █   ███ 
#                                     ██        
#--------------------------------------------------------------------------------------------------
Funkcje

Etykiety
# nadanie etykiety
kubectl label nazwaNoda nazwaChybaPoda test=true  
#                                      └ nasz label, zawierajacy klucz:wartość
#Pobranie etykiet
kubectl get nodes --show-labels

# Mozna skorzytac z etykiety:
# W przykładzie w kind: DaemonSet
# Automatyczne uruchamianie wszystkich podów, oznaczonych labelką. Trzeba dodać ta labelkę do:
spec.template.spec.nodeSelector:
                     test: "true"

#--------------------------------------------------------------------------------------------------
Init 
# Coś, co ma się wykonać przed uruchomieniem głównego kontenera. Np: podłączenie bo bazy, ściagnij plik > rozpakuj i dopiero uruchom
# Init dopiuje się w:
spec.template.spec.initContainers:
                   - name: unzip
                     image: garthk/unzip
                     command: [ "unzip", "/www/repo.zip", "-d", "/www/repo"]
                     volumeMounts:
                     - mountPath: /www
                       name: http-content
                   - name: clear
                     image: ubuntu
                     command: [ "sh", "-c", "rm -f /www/repo.zip" ]
                     volumeMounts:
                     - mountPath: /www
                       name: http-content

#--------------------------------------------------------------------------------------------------
Port-forward

kubectl port-forward nazwaUruchomionegoPoda 8080:80
# W odpowiedzi dostaniemy: Forwardng from 127.0.0.1:8080 -> 80 
# Będzie można zobaczyć wynik w przeglądarce 


# jakieś narzędzie
kubectl run -it --image=arunvelsriram/utils utils-container bash 

# ustawienie portu do nasluchu:
kubectl expose deployment hello-minikube --type=NodePort --port=8080


#--------------------------------------------------------------------------------------------------
key-value store


╔════════════╦═════════════╗
║ Key        ║ Value       ║
╠════════════╬═════════════╣
│ Name       │ john Doe    │
│ Age        │ 45          │
│ Location   │ New York    │
└────────────┴─────────────┘

Put Name "John Doe"
Get name             # => "John Doe"

Install ETCD:
# 1. Download Binaries
curl -L https://github.com/etcd-io/etcd/releases/download/v3.3.11/etcd-v3.3.11-linux-amd64.tar.gz -o etcd-v3.3.11-linux-amd64.tar.gz
# 2. Extract
tar xzvf etcd-v3.3.11-linux-amd64.tar.gz
# 3. Run ETCD Service
./etcd

# uruchomi się na porcie 2379. Powinno działac polecenie:
./etcdctl

# polecenie ustawienia nowego rekordu:
./etcdctl set key1 value1 
etcdctl put key1 value1   # z inngo przykładu
# odczyt
./etcdctl get key1  # => value1
# odczyt tylko klucza
etcdctl get / --prefix --keys-only


# Setup - Manual
wget -q --https-only\
    "https://github.com/coreos/etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz"
# podgląd ustawień:
etcd.service
# wejście do kontenera z ustawieniami:
kubectl exec etcd-master -n kube-system etcdctl get / --prefix -keys-only

# jakieś pytanie do ETCD
curl -X POST /api/v1/namespaces/default/pods ...[other]

# instalacja:
  kube-apiserver" \
  kube-controller-manager" \
  kube-scheduler" \
  kubectl"
https://discuss.kubernetes.io/t/control-plane-provisioning-failed/8323

# jakas inna istalacja etcd - Film 224   Certified Kubernetes Administrator (CKA) with Practice Tests


# Instalacja kube-apiserver - nie do końca wiem po co to instalować?
wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-apiserver
# sprawdzenie statusu Film 15  2:50 (Certified Kubernetes Administrator (CKA) with Practice Tests)
kube-apiserver.service



# Instalacja kube-controller-manager
wget https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kube-controller-manager
kube-controller-manager.service


instalacja Scheduler (ang. planista)
wget https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kube-scheduler
# powinien powstać plik kube-scheduler.service

/etc/kubernetes/manifests/kube-scheduler.yaml
┌───────────────────
apiVersion: v1
kind: Pod
metadata:
  name: kube-scheduler
  namespace: kube-system
spec:
  containers:
    command:
      kube-scheduler
      --address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
      #--sheduler-name=my-custom-sheduler   # opcja dla my-custom-scheduler.yaml
    image: k8s.gcr.io/kube-scheduler-amd64:v1.11.3
    name: kube-scheduler
└───────────────────

#--------------------------------------------------------------------------------------------------
#  █     █
#  ██   ██   █                                                       █
#  █ █ █ █        ███   █ ███   ███    ███    ███   █ ███  █     █        ███    ███    ███
#  █  █  █  ██   █   █  ██     █   █  █      █   █  ██     █     █  ██   █   █  █   █  █
#  █     █   █   █      █      █   █   ███   █████  █       █   █    █   █      █████   ███
#  █     █   █   █   █  █      █   █      █  █      █        █ █     █   █   █  █          █
#  █     █  ███   ███   █       ███    ███    ███   █         █     ███   ███    ███    ███
#--------------------------------------------------------------------------------------------------
Microservices  # film 44 (Kubernetes for the Absolute Beginners - Hands-on)

Tworzenie serwisu do głosowania
https://github.com/dockersamples/example-voting-app

# tworzymy kontener z redisem, port 6379 
docker run -d --name=redis redis
# tworzymy kontener z bazą danych, port 5432
docker run -d --name=db postgres:9.4
# plikacj Web dla frontu, na której jest formulaz do głosowania (napisany w Pythonie), port 80
docker run -d --name=vote -p 5000:80 --link redis:redis  voting-app
# aplikacja do wyswietlanai wyników (napisana w node.js), port 80
docker run -d --name=result -p 5001:80 --link db:db  result-app
# aplikacja która będzie zbierała głosy i zapisywała ja w bazie (napisana w .NET)
docker run -d --name=worker --link db:db --link redis:redis  worker

# kroki do osiągniecia celu:
1. Deploy PODs
2. Create Services (ClusterIP)
  1. redis
  2. db
3. Create Services (NodePort)
  1. voting-app
  2. result-app

# sprawdzenie czy się utworzył:
kubectl get pods,svc

# utworzenie tunelu, aby sprawdzić przez stronę, czy coś widać (w CMD w trybie administratora):
minikube service voting-service --url
# u mnie nie zadziałało











#--------------------------------------------------------------------------------------------------
# szyfrowanie haseł:
kubectl create secret generic pgpassword --from-literal PGPASSWORD=12345test

# odczyt tokenów z hasłami:
kubectl get secret

# Pogubiłem się na Filmie YT 17:20 https://www.youtube.com/watch?v=OVVGwc90guo
# Nie znalazłem linku instalacyjnego, takiego jak koles z filmu.

kubectl get pv

kubectl delete -f k8s



#--------------------------------------------------------------------------------------------------
# commands

# Film 95 Certified Kubernetes Administrator (CKA) with Practice Tests

FROM Ubuntu
CMD sleep 5   # lub ["sleep", "5"]
# podczas usruchamainia, możemy ustawić argument:
docker run ubuntu-sleeper sleep 10


FROM Ubuntu
ENTRYPOINT ["sleep"]
# aby przechwycić argument przekazany podczas wpisywania komenty:
docker run ubuntu-sleeper 10

FROM Ubuntu
ENTRYPOINT ["sleep"]
CMD ["5"]
# stworzenie wartości domylsnej, której nie trzeba podawac przy wywołaniu polecenia
docker run ubuntu-sleeper

# gdy chce dodać niezdefiniowany entrypoint podczas komendy:
docker run --entrypoint sleep2.0 ubuntu-sleeper 10


# Film 96  Certified Kubernetes Administrator (CKA) with Practice Tests
# Pod z ubynty, który ma nieazmyakc sie przez 5 sekund?
pod-definition.yaml
┌───────────────────
apiVersion:v1
kind:Pod
metadata:
  name:ubuntu-sleeper-pod
spec:
  containers:
  - name:ubuntu-sleeper
    image:ubuntu-sleeper
    command:["sleep2.0"]  # odpowiednik pola ENTRYPOINT ["sleep"]   w dokerze
    args:["10"]           # odpowiednik pola CMD ["10"]             w dokerze
└───────────────────
kubectl create -f pod-definition.yaml


#--------------------------------------------------------------------------------------------------
#                                           ███    ████       █    ███ 
#                                          █   █  █    █     ██   █   █
#   ████   ████   ███ ██    ███                █  █    █    █ █   █   █
#  █   █       █  █  █  █  █   █   █████      █   █    █   █  █    ███
#  █   █   █████  █  █  █  █████             █    █    █  ██████  █   █
#   ████  █    █  █  █  █  █                █     █    █      █   █   █
#      █   ███ █  █  █  █   ███            █████   ████       █    ███
#  ████
#--------------------------------------------------------------------------------------------------
# Dzialajacy przyklad.
# Zakladam, ze mam uruchomiony minikube.
# W dowolnym folderze. Tworze plik 'all-in-one.yml' z zawartoscią poniżej (oczywiscie nazwa dowolna)
# Z poziou tego folderu odpalam sobie konsole (np git bash) i wywołuje polecenia
kubectl apply -f all-in-one.yml
  #jeżeli wyskoczy blad "Unable to connect to the server..." to pewnie trzeba odpalic minikube: minikube start
kubectl proxy

# Dostęp do Podów przez przeglądarkę: 
http://localhost:8001/api/v1/namespaces/game/services/http:service-2048:/proxy/
# dla OVH zadziałało:
http://51.77.59.63:30008/

# Aby sprawdzic wszystkie odpalone "serwisy/usługi"? zwiazane z 
kubectl get all -n game

# Aby usunac calosc:
kubectl delete -f all-in-one.yml


# zawartosc pliku:
---
apiVersion: v1
kind: Namespace 
metadata:
  name: game                    # *1v
---
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: game               # *1^ musi byc jak wyzej
  name: deployment-2048
spec:
  selector:
    matchLabels:
      name: app-2048            # jakimi podami deployment ma sie zaopiekowac (wykrywa wszystkie pody o takiej nazwie)
  replicas: 3
  template:
    metadata:
      labels:
        name: app-2048          # jaka jest nazwa naszych podow
    spec:
      containers:
      - image: k8smaestro/2048  # obraz z dockerHub
        imagePullPolicy: Always # polityka pobierania obrazow na klaster (zawsze pobieraj obraz)
        name: app-2048
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  namespace: game               # *1^
  name: service-2048
spec:
  ┌──────────────────────────   to ustawienie działa stacjonarnie
  type: ClusterIP               # ClusterIP: udostepnij aplikacje tylko w obrebie klastra. (serwi otrzymuje unikalny IP tylko w sieci wewnetrznej )
  ports:
  - port: 80                    # jaki port ma byc wystawiony
    targetPort: 80              # port kontenera (containerPort z Deployment, do jakiego porty ma przekierowac ruch)
    protocol: TCP
  └──────────────────────────
  ┌──────────────────────────   to ustawienie działa na OVH (dla jednego noda)
  type: NodePort
  ports:
  - port: 80               # jaki port ma byc wystawiony
    targetPort: 80         # port kontenera (containerPort z Deployment, do jakiego portu ma przekierowac ruch)
    nodePort: 30008
  └──────────────────────────
  selector:
    name: app-2048


#--------------------------------------------------------------------------------------------------
                         #                                 #                                            #
        #                #                                 #                                            #
  ###      ### ##  ####  #      ###          #     #  ###  ####   ####  ####  ####           ###   ###  #      ###  # ###
 #     ##  #  #  # #   # #     #   #  #####  #     # #   # #   #      # #   # #   #  #####  #   # #   # #     #   # ##
  ###   #  #  #  # #   # #     #####         #  #  # ##### #   #  ##### #   # #   #         #     #   # #     #   # #
     #  #  #  #  # ####  #   # #             # # # # #     #   # #    # ####  ####          #   # #   # #   # #   # #
  ###  ### #  #  # #      ###   ###           #   #   ###  ####   ### # #     #              ###   ###   ###   ###  #
                   #                                                    #     #
#--------------------------------------------------------------------------------------------------
Działajacy simple-webapp-color na OVH

┌──────────────────────────
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-color-dep
spec:
  replicas: 1
  selector:
    matchLabels:
      name2: my-color
  template:
    metadata:
      labels:
        name2: my-color
    spec:
      containers:
      - name: web-color
        image: mmumshad/simple-webapp-color:latest
        ports:
        - containerPort: 8080
        env:
        - name: APP_COLOR
          value: red
---
apiVersion: v1
kind: Service
metadata:
  name: my-color-service
spec:
  type: NodePort
  ports:
  - port: 80               # jaki port ma byc wystawiony
    targetPort: 8080       # port kontenera (containerPort z Deployment, do jakiego portu ma przekierowac ruch)
    nodePort: 30010
  selector:
    name2: my-color
└──────────────────────────
Dostępny pod adresem: http://51.77.59.63:30010


apiVersion: v1
kind: Namespace
metadata:
  name: udemy                # ta sama nazwa musi byc podana w metadata.namespace
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config                     # *1v
data:
  APP_COLOR: black
  APP_MODE: prod
---
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: udemy
  name: web-color-dep
spec:
  replicas: 1
  selector:
    matchLabels:
      name2: my-color
  template:
    metadata:
      labels:
        name2: my-color
    spec:
      containers:
      - name: web-color
        image: mmumshad/simple-webapp-color:latest
        ports:
        - containerPort: 8080
#        env:    # zmienne srodowiskowe zdefiniowane wszystkie tutaj:
#        - name: APP_COLOR
#          value: blue

#        env:    # pojedyncza zmienne odczytana z ConfigMap
#        - name: APP_COLOR
#          valueFrom:
#            configMapKeyRef:
#              name: app-config
#              key: APP_COLOR

        envFrom: # kilka zmiennych odczytanych z ConfigMap
        - configMapRef:
            name: app-config        # *1^
---
apiVersion: v1
kind: Service
metadata:
  namespace: udemy
  name: my-color-service
spec:
  type: NodePort
  ports:
  - port: 80               # jaki port ma byc wystawiony
    targetPort: 8080       # port kontenera (containerPort z Deployment, do jakiego portu ma przekierowac ruch)
    nodePort: 30010
  selector:
    name2: my-color


#--------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------
# Opis, jak wstawić projekt na google cloud
# Film 49 (Kubernetes for the Absolute Beginners - Hands-on)
https://cloud.google.com/free/docs/gcp-free-tier

# 1. menu -> Kuberbetes Engin
# 2. Clusters -> Create cluster
# 3. Wpisujemy indywidualną nazwę, reszta ustawień domyślna.  -> Create  (może to portrwać kilka minut)
# 4. W części "Cluster" pojawi się nowa pozycja z zielonym znaczkiem statusu. -> Connect -> na dole powinna otworzyć się konsola.
# 5. W konsoli powinno pojawić się domyślne polecenie do konfiguracji połaczenia
gcloud container clusters get-credentials example-voting-app --zone us-central1-c --project example-voting-app-283506
# 6. W przykłądzie, projekt jest na githubie
https://github.com/kodekloudhub/example-voting-app
# 7. W konsoli chmury, kopiuje projekt
git clone https://github.com/kodekloudhub/example-voting-app.git
# 8. Po skopiowaniu projektu, wchodzi do folderu projektu, następnei do folderu k8s-specifications z plikami xxx-deploy.yaml i xxx-service.yaml
# 9. W plikach voting-app-service.yaml i result-service.yaml, pozmieniał spec: type: LoadBalancer i usunięty jest "nodePort"
# 10. W konsoli chmury, uruchamiam deploymenty i servisy:
kubectl create -f voting-app-deploy.yaml
kubectl create -f voting-app-service.yaml
    # to samo dla redis, postgres, worker, result
# 11. Po wejściu w zakładkę Kubernetes Engine -> Services & Ingress, 
    # powinienem widzieć uruchomione serwisy i ich statusy
    # Po kliknięciu do Endpoinds, powinno odesłać nas na stronę, gdzie będą widoczne rezultaty

#--------------------------------------------------------------------------------------------------
# Opis, jak wstawić projekt na AWS
# Film 51 (Kubernetes for the Absolute Beginners - Hands-on)
https://docs.aws.amazon.com/eks/latest/userguide/getting-started-console.html
# 1. Po zalogowaniu się, przechodzimy do AWS Managment Console
# 2. Wysukujemy i tworzymy EKS (Elastic Kubernetes Service) 
#   - nadajemy nazwę
#   - W polu Cluster configuration -> Cluster Service Role -> wybieramy EKSRole -> Next
#   - Reszta ustawień domyślna. Kilka razy Next i tworzymy klaster. Może to potrwac kilka minut
# 3. Wchodzimy w EKS > Clusters > nazwa_projektu  i tutaj mamy panel naszego projektu
# 4. W lokalnej konsoli wpisuje polecenie: (chyba w lokalnej)
aws eks --region us-west-2 update-kubeconfig --name example-voting-app
# po wykoaniu komendy 
kubectl get nodes
# powinny pojawic się węzly z opsem: xxx.us-west-2.compute.internal
# reszta poleceń, odbywa sie chyba lokalnie??
# resztę tylko obejżałem...




#--------------------------------------------------------------------------------------------------
  #     #                #                   #      ####
  #     #   #            #                   #      #   #
  #     #       # ###  #####  #   #   ####   #      #   #   ###   #   #
  #     #  ##   ##       #    #   #       #  #      ####   #   #   # #
   #   #    #   #        #    #   #   #####  #      #   #  #   #    #
    # #     #   #        #    #   #  #    #  #   #  #   #  #   #   # #
     #     ###  #         ##   ####   ### #   ###   ####    ###   #   #
#--------------------------------------------------------------------------------------------------
# instalacja VirtualBox na Widowsie - Film 11 7:50 (Kubernetes for the Absolute Beginners - Hands-on)
# Odpaliłem sobie Cygwin64 Terminal:
# Przeprowadziłem instalację ma Windows według: https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/
Install kubectl binary with curl on Windows:
curl -LO "https://dl.k8s.io/release/v1.23.0/bin/windows/amd64/kubectl.exe"

Już mi działa polecenie:
kubectl version --client --output=yaml

Zainstalowałem choco (w PowerShelu), pleceniem:
Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))

Zainstalowałem kubernetes-cli:
choco install kubernetes-cli

W nowym oknie PowerShel działa polecenie:
kubectl version --client


Uruchamiam PowerShell w trubie administratora i wpisuje polecenia:
New-Item -Path 'c:\' -Name 'minikube' -ItemType Directory -Force
Invoke-WebRequest -OutFile 'c:\minikube\minikube.exe' -Uri 'https://github.com/kubernetes/minikube/releases/latest/download/minikube-windows-amd64.exe' -UseBasicParsing

$oldPath = [Environment]::GetEnvironmentVariable('Path', [EnvironmentVariableTarget]::Machine)
if ($oldPath.Split(';') -inotcontains 'C:\minikube'){ `
  [Environment]::SetEnvironmentVariable('Path', $('{0};C:\minikube' -f $oldPath), [EnvironmentVariableTarget]::Machine) `
}

Gdy wywołam polecenie:
minikube start --driver=virtualbox
  # Otrzymuje błąd: This computer doesn't have VT-X/AMD-v enabled. Enabling it in the BIOS is mandatory 
Gdy wywołam polecenie:
minikube start --driver=docker
  # Niby jest Done, ale po tym sypie błedami...

Po sprawdzeniu statusu:
minikube status 
  # otrzymałem statusy Runing (czyli dobrze)
  # Jeśli otrzymam Stoped, to trzeba go uruchomić poleceniem  'minikube start' w PowerShelu

Jeśli status jest Runing, działaja polecenia w PowerShel oraz w Cygwin:
kubectl get po -A
  # kilka linijek: kube-system   coredns-64897985d-wx2jf            1/1     Running   1 (2m34s ago)   67m 
kubectl get nodes
  # minikube   Ready    control-plane,master   77m   v1.23.3
kubectl cluster-info
  # CoreDNS is running at https://127.0.0.1:58175/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy




4. Zakładam przykładowa aplikację:
kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4
Ustawiam dostęp:
kubectl expose deployment hello-minikube --type=NodePort --port=8080

Przy prubie pobrania tunelu, konsola zostaje zawieszona:
minikube service hello-minikube --url
  # http://192.168.49.2:30535   - teoretycznie, pod tym adresem powinienem widzieć coś.

Usunięcie serwisu hello-minikube:
kubectl delete services hello-minikube
Usunięcie deployment hello-minikube:
kubectl delete deployment hello-minikube


Komendy dostepne dla CLUSTER:
minikube pause
minikube unpause
minikube stop
minikube delete --all




kubectl run myNginx --image=nginx  # Stworzy POD z kontenerem nginxa
kubectl get pods -o wide 
  # NAME    READY   STATUS    RESTARTS   AGE     IP           NODE       NOMINATED NODE   READINESS GATES
  # mtNginx 1/1     Running   0          5m22s   172.17.0.3   minikube   <none>           <none>
kubectl describe pod nginx   # wyświetl szczegóły konkretnego poda

# wyswietlenie konkretnej wartości:
kubectl get pods -o=jsonpath='{.items[0].spec.containers[0].image}'

# wyswietlenie konkretnej wartości ze wszystkich elementów tablicy:
kubectl get pods -o=jsonpath='{.items[*].metadata.name}'

# wyswietlenie konkretnej wartości ze wszystkich elementów dwóch tablic:
kubectl get pods -o=jsonpath='{.items[*].metadata.name}{.items[*].status.capacity.cpu}'

# wyswietlenie konkretnej wartości ze wszystkich elementów dwóch tablic, rozdzielone tabem:
kubectl get pods -o=jsonpath='{.items[*].metadata.name}{"\t"}{.items[*].status.capacity.cpu}'

# Loops - range, wyświetl pary wartości obok siebie i rozdziel enterem 
kubectl get pods -o=jsonpath='{range .items[*]} {.metadata.name}{"\t"}{.status.capacity.cpu}{"\n"}{end}'

# wyświetlenie w formie kolumn
kubectl get nodes -o=custom-columns=<COLUMN NAME>:<JSON PATH>
kubectl get nodes -o=custom-columns=NODE:.metadata.name ,CPU:..status.capacity.cpu

# sortowanie według okreslonej własciwości
kubectl get nodes --sort-by= .metadata.name
kubectl get nodes --sort-by= .status.capacity.cpu



#--------------------------------------------------------------------------------------------------
#  #     #                       #              #
#  ##   ##    #             #    #              #
#  # # # #         ####          #  #   #   #   ####     ### 
#  #  #  #   ##    #   #   ##    # #    #   #   #   #   #   #
#  #     #    #    #   #    #    ##     #   #   #   #   #####
#  #     #    #    #   #    #    # #    #   #   #   #   #    
#  #     #   ###   #   #   ###   #  #    ####   ####     ### 
#--------------------------------------------------------------------------------------------------
#                          _             _
#             _         _ ( )           ( )
#   ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
# /' _ ` _ `\| |/' _ `\| || , <  ( ) ( )| '_`\  /'__`\
# | ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
# (_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)
#--------------------------------------------------------------------------------------------------

MINIKUBE
# Uruchomienie minikube na windowsie.
# Trzeba odpalić CMD w trybie administratora i uruchomic polecenie: (zakładam ze mam zainstalowany hyperV)
minikube start --driver=hyperv

#od tej pory powinny działać polecenia:
minikube start
minikube stop

# aby skasować minikube:
minikube delete


minikube CLI  - for start ip/deleting the cluster
kubectl CLI   - for configuring the minikube cluster

# odpalenie minikube w trybie debag
minikube start --vm-driver=hyperkit --v=7 --alsologtostderr


# Instalacja minikube
1. Installation:
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube
    na filmie 11 koles miał inne polecenia:
    curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
    chmod +x minikube
    ls -ld /usr/local/bin/

2. Start your cluster:
minikube start
minikube start --driver=ssh --ssh-ip-address=10.10.10.186

    Na sterfie kursów, kongiguracja nastapiła przez polecenie:
    minikube config set driver docker

    Na moim Windowsie, działa konfiguracja:
    minikube start --driver=hyperv


3. gdy sie zainstaluje, można spawdzic:
minikube status


#-------------------
# W filmie 2, kubectl instalacja (Strefa Kursów Kurs: Kubernetes od podstaw - zarządzanie i automatyzacja kontenerów)
# Koles instaluje kubectl poleceniem:
curl -LO "https://storage.googleapis.com/kubernetes-relase/relase/$(curl -s curl https://storage.googleapis.com/kubernetes-relase/relase/stable.txt)/bin/linux/am64/kubectl"

# następnie, trzeba nadac uprawnienia
chmod +x ./kubectl

# przenieść plik do sciezki projektu ??
sudo mv ./kubectl /usr/local/bin/kubectl

#Sprawdzenie wersji:
kubectl version --client



# Po zainstalowanieu minikube, mozna do niego wejść poleceniem:
$ minikube ssh

#aby wyjść z tego trybu:
exit

# działa tu polecenie:
docker ps
docker container ls

# aby śledzić jakiś pod, znajac jego adres
watch -n 1 "curl -s 172.17.0.7"


# aby podejżec plik konfguracyjny (w Windowsie). uruchom gitbash w trybie administratora.
# po wspisaniu pwd, powinienem mieć ścieżkę: /c/Users/Humansoft
# po wpisaniu ls -la  powinienem widzieć plik .kube
# Konfiguracja jest w pliku config, który można podejżeć, pn: za pomocą nano lub less:
nano .kube/config


# Po wydaniu polecenia
minikube dashboard
# powinno nam się udac wejsc na stroę:
http://127.0.0.1:46257/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/



#--------------------------------------------------------------------------------------------------
  #     #                       #                        #   #                           # 
  ##   ##                 #     #                       #    #                            #
  # # # #   ###   ####        #####   ###   # ###       #    #       ###    ####   ###    #
  #  #  #  #   #  #   #  ##     #    #   #  ##          #    #      #   #  #   #  #       #
  #     #  #   #  #   #   #     #    #   #  #           #    #      #   #  #   #   ###    #
  #     #  #   #  #   #   #     #    #   #  #           #    #   #  #   #   ####      #   #
  #     #   ###   #   #  ###     ##   ###   #            #    ###    ###       #   ###   # 
                                                                          ####
#--------------------------------------------------------------------------------------------------
Przykładowe monitory: Metric server, Prometeus, Elastic Stack, DATADOG, dynatrace

# uruchomienie Metric Server dla minikube: (Film 80   Kubernetes Certified Application Developer (CKAD) with Tests)
minikube addons enable metrics-server

# uruchomienie Metric Server dla pozostałych:
git clone https://github.com/kubernetes-incubator/metrics-serve
kubectl create -f deploy/1.8+/

kubectl top node

# jakas symulacja logowania.  Z opcja -d będzie działał w tle
docker run kodekloud/event-simulator

event-simulator.yaml
┌──────────────────────────
apiVersion: vl
kind: Pod
metadata:
  name: event-simulator-pod
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
  #- name: image-processor
  #  image: some-image-processor
└──────────────────────────

#odpalenie symulatora:
kubectl create -f event-simulator.yaml

# podglad logów na żywo
kubectl logs -f event-simulator-pod
kubectl logs -f event-simulator-pod event-simulator # gdy mamy dwa kontenery, to trzeba sprecyzować nazwę



#--------------------------------------------------------------------------------------------------
    ###                                            #
   #   #                                    #      #
   #        ###     ###    #   #   # ###         #####   #    #
    ###    #   #   #   #   #   #   ##      ##      #     #   #
       #   #####   #       #   #   #        #      #      # #
   #   #   #       #   #   #   #   #        #      #       #
    ###     ###     ###     ####   #       ###      ##    #
                                                         #
#--------------------------------------------------------------------------------------------------
Security

kubectl get serviceaccount
kubectl create serviceaccount sa1

Mechanizmy autentykacji:
Static Password File
Static Token File
Certificates
Identity Services

# TLS 
# Asymetric encryption - SSH


ssh-keygen  # hyba polecenie do generowania klucza prywatnego i publicznego
# klucze powinny być w cat ~/.ssh/authorized_keys
openssl genrsa -out nazwa-pliku.key 1024                # hyba polecenie do generowania pliku z kluczem publicznym
openssl rsa -in my-bank.key -pubout > nazwa-pliku.pem   # hyba polecenie do generowania kompletu kluczey

Certificate Signing Request (CSR)  
openssl req -new -key my-bank.key -out my-bank.csr -subj "/C=US/ST=CA/O=MyOrg, Inc./CN=my-bank.com"

Certyfikaty publiczne mają rozszeżenia .ctr i .perm
Certyfikaty prywatne  mają rozszeżenia .key i -key.perm

Rodzaje Certyfikatów: Root, Client, Server
Certyfikaty trafiajace do KUBE-API: admin.crt, scheduler.crt, controller-manager.crt, kube-proxy.crt

# generowanie OPENSSL     Film 142 Certified Kubernetes Administrator (CKA) with Practice Tests
# certificate authority (CA)
openssl genrsa -out ca.key 2048                                    # generate keys
openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA" -out ca.csr # Certificate signing request
poenssl x509 -req -in ca.csr -signkey -out ca.crt                  # Sign certificates
# admin user
openssl genrsa -out admin.key 2048                                       # generate keys
openssl req -new -key admin.key -subj "/CN=kube-admin" -out admin.csr    # Certificate signing request
poenssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt  # Sign certificates
# kubescheduler, controller, proxy...
curl https:// kube-apiserver:6443/api/v1/pods --key admin.key --cert admin.crt --cacert ca.crt

kube-config.yaml
┌───────────────────
apiVersion:vl
clusters:
- cluster:
    certificate-authority: ca.crt
    server: https:// kube-apiserver:6443
  name: kubernetes
kind: Config
users:
  name: kubernetes-admin
  user:
    client-certificate: admin.crt
    client-key: admin.key
└───────────────────

# jakieś informacje o certyfikatach dla kubeadm: /etc/kubernetes/manifests/kube-apiserver.yaml
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout  # polecenie do podglądu własicwości certyfikatów
journalctl -u etcd.service -l                                    # jakies logi

# generowanie certyfikatu dla kolejnego administratora Film 146 Certified Kubernetes Administrator (CKA) with Practice Tests
openssl genrsa -out jane.key 2048
openssl req -new-key jane.key -subj"/CN=jane"-outj

jane-csr.yaml
┌───────────────────
apiVersion: certificates.k8s.io/v1beta1
kind: CertificateSigningRequest
metadata:
  name: jane
spec:
  groups:
  - system:authenticated
  usages:
  - digital signature
  - key encipherment
  - server auth
  request:
    LSdakSKDFJfKJFK  # zawartoć pliku, która przeszła przez polecenie:  cat jane.csr | base64 | tr -d "\n"
└───────────────────

# jakies dane o ertyfikacie
curl https://localhost:6443 -k
curl https://localhost:6443 -k --key admin.key --cert admin.crt --cacert ca.crt


#--------------------------------------------------------------------------------------------------
kontenery do testów:
kodekloud/random-error            # co jakiś czas generuje kontener, który ma błąd
reporting-tool 


#--------------------------------------------------------------------------------------------------
#--------------------------------------------------------------------------------------------------

Dckerization NestJS
Filmik https://www.youtube.com/watch?v=ra_kJFIpU4A

Pirewsza instalacja aws CLI
https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html

Gdy miałem problem z komunikatem:
e connection to the server localhost:8080 was refused - did you specify the right host or port?
W konsoli na stronie, wpisałem:
aws eks --region eu-central-1 update-kubeconfig --name keszua


Instalowanie pakietów w AWS CloudShell:
sudo yum install mc -y


Działający Dockerfile

┌───────────────────
ARG IMAGE=node:16.13-alpine

#COMMON
FROM $IMAGE as build

WORKDIR /app
COPY ./package*.json .
RUN npm i
COPY . .
RUN npm run build


#PRODUCTION
FROM $IMAGE
WORKDIR /app
COPY ./package.json .
RUN npm install --only=production

COPY --from=build ./app/dist ./dist
CMD npm run start:prod

└───────────────────
# stworzenie obrazu:
# docker build -t app .

# testowe uruchomienie kontenera
# docker run -p 8888:80 app


Kontener odpalam na AWS w usłudze EKS

Storzyłem klaser keszua
Gdy się stworzy, w "Compute" dodaje node "Add node group"

Typ instancji
t3.medium
vCPU: 2 vCPUs
Memory: 4 GiB
Network: Up to 5 Gigabit
Max ENI: 3
Max IPs: 18



AWS CloudShell
# Pobranie pliku z chmury:
# sprawdzic swoją ścierzkę
pwd
# Actions -> Download file -> wkleić ścierzkę ome/cloudshell-user/plik


# Polecenia do serwisowania:
# volumen:
kubectl describe pvc mysql-pv-claim
# baza
kubectl describe deployment mysql

